{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "772ee4b0",
   "metadata": {},
   "source": [
    "<center><a href=\"https://www.nvidia.cn/training/\"><img src=\"https://dli-lms.s3.amazonaws.com/assets/general/DLI_Header_White.png\" width=\"400\" height=\"186\" /></a></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec891b24",
   "metadata": {},
   "source": [
    "# <font color=\"#76b900\"> **7:** 编排与智能体</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2435db17",
   "metadata": {},
   "source": [
    "欢迎回到课程！在之前的 notebook 中，我们探索了 LLM 服务，部署了能够处理复杂指令并进行对话的模型。我们学习了如何通过各种接口与这些模型进行交互，并简要介绍了 LLM 编排。\n",
    "\n",
    "现在，我们将更深入地探讨如何编排 LLM 来构建复杂的应用。可以把 LLM 编排看作是指挥一场交响乐，不同的组件——提示词、检索机制、路由策略和工具——都和谐地协作，构建出强大的终端应用。\n",
    "\n",
    "#### **学习目标：**\n",
    "\n",
    "到本 notebook 结束时，您将能够：\n",
    "- 理解如何处理严肃的 LLM 任务，比如长篇推理和生成。\n",
    "- 学习提升上下文和改善 LLM 输出的检索技术。\n",
    "- 探索将任务导向适当模型或工具的路由策略。\n",
    "- 学习如何集成外部工具来扩展 LLM 的能力。\n",
    "- 开发能够进行推理和迭代行动的智能系统，最终形成 ReAct 循环。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aeb8cdf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: NVIDIA_BASE_URL=http://nim:8000/v1\n",
      "env: NVIDIA_DEFAULT_MODE=open\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "\n",
    "## USE THIS ONE TO START OUT WITH. NOTE IT'S INTENTED USE AS A VISUAL LANGUAGE MODEL FIRST\n",
    "# model_path=\"http://localhost:9000/v1\"\n",
    "## USE THIS ONE FOR GENERAL USE AS A SMALL-BUT-PURPOSE CHAT MODEL BEING RAN LOCALLY VIA NIM\n",
    "model_path=\"http://nim:8000/v1\"\n",
    "# ## USE THIS ONE FOR ACCESS TO CATALOG OF RUNNING NIM MODELS IN `build.nvidia.com`\n",
    "# model_path=\"http://llm_client:9000/v1\"\n",
    "\n",
    "model_name = requests.get(f\"{model_path}/models\").json().get(\"data\", [{}])[0].get(\"id\")\n",
    "%env NVIDIA_BASE_URL=$model_path\n",
    "%env NVIDIA_DEFAULT_MODE=open\n",
    "\n",
    "if \"llm_client\" in model_path:\n",
    "    model_name = \"meta/llama-3.1-70b-instruct\"\n",
    "\n",
    "llm = ChatNVIDIA(model=model_name, base_url=model_path, max_tokens=5000, temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c244dd4",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<br>\n",
    "\n",
    "## **7.1：** 构建 LLM 工作流\n",
    "\n",
    "提示工程是设计输入的艺术与科学，旨在引导 LLM 生成期望的输出。由于这些模型本质上是**随机鹦鹉（stochastic parrots）**——也就是说，它们基于输入和训练数据输出概率性响应——您可以预期某些任务对最强大的模型来说会容易得多。幸运的是，我们确切知道这些模型最擅长的是什么：**“摘要”或合成类型的任务**。\n",
    "\n",
    "**擅长短文本生成的原因其实很简单：**\n",
    "- 长文本生成难以持续追踪，LLM 可能会因自回归抽样的累积误差而偏离方向。\n",
    "- 大多数开发者希望节省生成长度，更愿意接受过短的响应，而不是意外过长的响应。\n",
    "- 在聊天应用中（通常是最流行的默认模型，也是最常用的指令格式），较短、简洁和“像摘要一样”的响应在实际场景中更受欢迎。\n",
    "\n",
    "**话虽如此，长上下文推理却被高度重视：**\n",
    "- 指定长输出以帮助强化生成风格/格式的能力，从最终用户的角度来看是非常有吸引力的。\n",
    "- 即使响应的生成先验倾向于短输出，短输出的累积也可能迅速变得非常长。\n",
    "\n",
    "**基于这些原因，大多数模型往往被训练用于短文本生成并吸收长段的上下文。** 这使得 LLM 非常适合诸如摘要和长文本问答等任务，本质上归结为**知识合成**或**蒸馏（distillation）**问题。这个概念在之前的 notebook 中已经用代码进行了探讨，但在今后的学习中，正式化并牢记这一点是非常重要的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6f3c828",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is a summary of the course, covering each notebook:\n",
      "\n",
      "**Notebook 1: JupyterLab and Jupyter Notebooks**\n",
      "\n",
      "In this notebook, we introduced the JupyterLab interface and explored its features, including the Launcher page, file browser, and main workspace. We also learned how to create and run Jupyter notebooks, and how to use the `Shift+Enter` key to execute code cells.\n",
      "\n",
      "**Notebook 2: LLM Introduction**\n",
      "\n",
      "In this notebook, we introduced the concept of Large Language Models (LLMs) and their applications. We learned about the HuggingFace library and how to use it to load and interact with LLMs. We also explored the architecture of BERT and its variants, and how they are used for natural language processing tasks.\n",
      "\n",
      "**Notebook 3: LLM Encoder and Decoder**\n",
      "\n",
      "In this notebook, we delved deeper into the architecture of LLMs and explored the encoder and decoder components. We learned about the transformer architecture and how it is used in LLMs. We also implemented a simple text classification task using a pre-trained LLM.\n",
      "\n",
      "**Notebook 4: LLM Encoder-Decoder and Only Decoder**\n",
      "\n",
      "In this notebook, we explored the use of LLMs for sequence-to-sequence tasks, such as machine translation and text summarization. We learned about the encoder-decoder architecture and how it is used in LLMs. We also implemented a simple machine translation task using a pre-trained LLM.\n",
      "\n",
      "**Notebook 5: Multi-Modal Architectures and Fusion Techniques**\n",
      "\n",
      "In this notebook, we explored the use of LLMs for multi-modal tasks, such as image and text classification. We learned about the use of different modalities, such as text, image, and audio, and how they can be combined using fusion techniques. We also implemented a simple image classification task using a pre-trained LLM.\n",
      "\n",
      "**Notebook 6: LLM Server Introduction**\n",
      "\n",
      "In this notebook, we introduced the concept of LLM servers and how they can be used to deploy and access LLMs. We learned about the vLLM project and how it can be used to deploy and access LLMs. We also implemented a simple LLM server using the vLLM project.\n",
      "\n",
      "**Notebook 7: Agentics and Smart Bodies**\n",
      "\n",
      "In this notebook, we explored the concept of agentics and smart bodies, and how they can be used to create more complex and interactive systems. We learned about the use of LLMs as a component of smart bodies, and how they can be used to create more intelligent and autonomous systems. We also implemented a simple smart body using a pre-trained LLM.\n",
      "\n",
      "**Notebook 8: Assessment**\n",
      "\n",
      "In this notebook, we implemented a simple assessment task that combined the concepts learned throughout the course. We created a system that could generate images based on text prompts, and used a pre-trained LLM to generate the text prompts.\n",
      "\n",
      "In terms of big-picture ideas, the course covered the following topics:\n",
      "\n",
      "* The basics of JupyterLab and Jupyter notebooks\n",
      "* The architecture and applications of LLMs\n",
      "* The use of LLMs for natural language processing tasks\n",
      "* The use of LLMs for sequence-to-sequence tasks\n",
      "* The use of LLMs for multi-modal tasks\n",
      "* The concept of agentics and smart bodies\n",
      "* The use of LLMs as a component of smart bodies\n",
      "\n",
      "To help an instructor explain the material and understand which parts of the course to refer to when addressing questions, the following key points can be emphasized:\n",
      "\n",
      "* The course covers the basics of JupyterLab and Jupyter notebooks, which are essential for working with LLMs.\n",
      "* The course introduces the concept of LLMs and their applications, and explores the architecture and use of LLMs for natural language processing tasks.\n",
      "* The course covers the use of LLMs for sequence-to-sequence tasks, such as machine translation and text summarization.\n",
      "* The course explores the use of LLMs for multi-modal tasks, such as image and text classification.\n",
      "* The course introduces the concept of agentics and smart bodies, and explores the use of LLMs as a component of smart bodies.\n",
      "* The course provides a simple assessment task that combines the concepts learned throughout the course.\n",
      "\n",
      "By emphasizing these key points, an instructor can help students understand the big-picture ideas and concepts covered in the course, and provide a clear and concise overview of the material."
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from chatbot.jupyter_tools import FileLister\n",
    "\n",
    "import os \n",
    "filenames = [v for v in sorted(os.listdir(\"temp_dir\")) if v.endswith(\".ipynb\")]\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", (\n",
    "        \"You are a helpful DLI Chatbot who can request and reason about notebooks.\"\n",
    "        \" Be as concise as necessary, but follow directions as best as you can.\"\n",
    "        \" Please help the user out by answering any of their questions and following their instructions.\"\n",
    "    )),\n",
    "    (\"human\", \"Here is the notebook I want you to work with: {full_context}. Remembering this, start the conversation over.\"),\n",
    "    (\"ai\", \"Awesome! I will work with this as context and will restart the conversation.\"),\n",
    "    (\"placeholder\", \"{messages}\")\n",
    "])\n",
    "\n",
    "def compute_context(state: dict):\n",
    "    return FileLister().to_string(files=state.get(\"filenames\"), workdir=\".\")\n",
    "\n",
    "pipeline = (\n",
    "    RunnablePassthrough.assign(full_context = compute_context)\n",
    "    | chat_prompt \n",
    "    | llm \n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "chat_state = {\n",
    "    # \"filenames\": [\"07_intro_agentics.ipynb\"],\n",
    "    # \"messages\": [(\"human\", \"Can you give me a summary of the notebook?\")],\n",
    "    ## Reason about the entire course at once. This will be much slower and does not scale to larger document pools. \n",
    "    \"filenames\": filenames, \n",
    "    \"messages\": [(\"human\", \n",
    "        \"Can you give me a summary of the course, making sure to mention every notebook?\"\n",
    "        \" Do a paragraph per notebook, and finish by explaining big-picture ideas to help an\"\n",
    "        \" instructor explain the material and understand which parts of the course to refer to when addressing questions.\"\n",
    "    )],\n",
    "}\n",
    "\n",
    "short_summary = \"\"\n",
    "for chunk in pipeline.stream(chat_state):\n",
    "    print(chunk, end=\"\")\n",
    "    short_summary += chunk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01634628",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 摆脱限制\n",
    "\n",
    "尽管现代模型有其自然倾向，但人们仍然能够通过多种可扩展的方法完成复杂的任务，比如长文本生成甚至更长的上下文摄取。**这些技术依赖于一个关键假设，即 LLM 可以被程序化地、重复地、独立地和并行地调用（服务器部署正好可以实现这些）。**\n",
    "\n",
    "<div><img src=\"imgs/data-pipelines.png\" width=\"800\"/></div>\n",
    "\n",
    "#### **迭代生成** \n",
    "\n",
    "对话实际上是一个长文本的逐步生成任务（其中一半的生成由人类完成），因此可以一次生成一段任意长度的文档。如果一个 LLM 只能一次输出 5000 个 token，但可以同时推理 100,000 个 token，那是不是可以一步一步生成一整篇文档呢？\n",
    "- **全上下文：** 如果迭代生成保持在模型的输入上下文限制内，那么保持对话历史、任务历史、文档历史等的全上下文方法可能就足够了。\n",
    "- **运行状态：** 当长上下文积累不受欢迎时，从之前生成中获取的运行摘要或其它类型的累积历史可能就足够了。这通常被称为 **“迭代优化（iterative refinement）”**，并且有保持**全局上下文**（或至少是回顾上下文，全局上下文可以通过预处理获得）的好处。\n",
    "- **滑动窗口：** 如果你想要摘要一个文档，只关心**局部上下文**（比如为了边界一致性，boundary consistency），您可以选择放弃整体历史，只考虑一段内容的窗口。这在**翻译任务**中尤其相关，因为你可以在翻译一个文档片段的同时记住之前翻译的部分和后面尚未翻译的部分。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ff56a1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00_jupyterlab.ipynb\n",
      " - JupyterLab Interface: Introduction to JupyterLab interface, including menu bar, file browser, and main workspace. Topics: JupyterLab, interface, file browser, notebook, kernel, environment.\n",
      " - Clearing GPU Memory: Methods to clear GPU memory, including soft reset, hard stop, and code unit to reset kernel. Topics: GPU, memory, reset, kernel, notebook.\n",
      " - Important Code: `print()` function, `Shift+Enter` to execute code, `IPython.Application.instance()` and `app.kernel.do_shutdown(True)` to reset kernel.\n",
      " - Connections to previous notebooks: None, this is the first notebook in the course.\n",
      " - Relevant Images: jl_launcher.png, showing JupyterLab interface.\n",
      "************************************************************************************\n",
      "01_llm_intro.ipynb\n",
      " - 1.1. 回顾深度学习: Review of deep learning basics, extension to language modeling, including linear regression, logistic regression, stacked linear layers, non-linear activation functions, convolutional layers, pre-trained components (e.g. VGG-16/ResNet), and high-performance data science with RAPIDS.\n",
      " - 1.2. 获取我们的第一个 LLM: Introduction to HuggingFace, a community for accessing, uploading, and testing/deploying large deep learning models, with a focus on large language models (LLMs), including the `bert-base-uncased` model and its pipeline.\n",
      " - 1.3. 剖析 Pipeline: Analysis of the pipeline, including the `transformers` package, the `tokenizer` and `model` components, and the `preprocess`, `forward`, and `postprocess` stages.\n",
      " - 1.4. 您的课程环境: Overview of the course environment, including the use of pre-loaded models, system memory, GPU, and GPU memory, with a focus on the importance of these resources for language modeling.\n",
      " \n",
      "Main Ideas and Relevance To Course: This notebook introduces the basics of language modeling, including the use of pre-trained models and pipelines, and sets the stage for the rest of the course, which will explore the inner workings of these systems and how to choose and use them effectively.\n",
      "\n",
      "Important Code: `from transformers import pipeline`, `AutoTokenizer`, `BertTokenizer`, `BertModel`, `FillMaskPipeline`, `AutoModelForMaskedLM`, `BertForMaskedLM`, `BertForPreTraining`, `MyMlmPipeline` class, `unmasker = pipeline('fill-mask', model='bert-base-uncased', device='cuda')`, `unmasker(\"Hello I'm a [MASK] model.\")`, and `IPython.Application.instance().kernel.do_shutdown(True)`.\n",
      "\n",
      "Connections to previous notebooks: None, this is the first notebook in the course.\n",
      "\n",
      "Relevant Images: `imgs/machine-learning-process.jpg`, showing the machine learning process, and `imgs/DLI_Header_White.png`, showing the DLI header.\n",
      "************************************************************************************\n",
      "02_llm_intake.ipynb\n",
      " - 2.1. 获取模型输入: Introduction to the input process of language models, including tokenization, embedding, and attention, with a focus on the `preprocess` and `postprocess` stages of the pipeline, and the use of the `tokenizer` and `model` components.\n",
      " - 2.2. 捕获 Token 语义: Exploration of token embeddings, including word embeddings, position embeddings, and token type embeddings, and the use of cosine similarity, scaled dot product similarity, and softmax similarity to analyze the embeddings.\n",
      " - 2.3. 从 token 级别的推理到文章级别的推理: Discussion of how language models perform reasoning at the token level and the article level, including the use of self-attention and multi-headed attention, and the role of the `transformer` architecture in enabling this reasoning.\n",
      " - Main Ideas and Relevance To Course: This notebook introduces the concept of token embeddings and their role in language modeling, and explores the attention mechanism used in transformer architectures to enable reasoning at the token and article levels.\n",
      " - Important Code: `from transformers import pipeline`, `unmasker = pipeline('fill-mask', model='bert-base-uncased')`, `tokenizer = unmasker.tokenizer`, `embeddings = model.bert.embeddings.word_embeddings(token_ids)`, `cosine_similarity`, `scaled_dp_similarity`, `softmax_similarity`, and `plot_mtx`.\n",
      " - Connections to previous notebooks: This notebook builds on the introduction to language models and pipelines in the previous notebook, and provides a deeper dive into the inner workings of language models.\n",
      " - Relevant Images: `imgs/attention-logic.png` and `imgs/bert-construction.png`, showing the attention mechanism and the BERT architecture.\n",
      "\n",
      "This notebook is a crucial step in understanding the inner workings of language models, and provides a solid foundation for the rest of the course, which will explore more advanced topics in language modeling. The notebook introduces the concept of token embeddings and their role in language modeling, and explores the attention mechanism used in transformer architectures to enable reasoning at the token and article levels. The notebook also provides a deeper dive into the BERT architecture, and introduces the concept of self-attention and multi-headed attention. The connections to the previous notebook are strong, as this notebook builds on the introduction to language models and pipelines, and provides a deeper dive into the inner workings of language models.\n",
      "************************************************************************************\n",
      "03_encoder_task.ipynb\n",
      " - 3.1. 用于预测 token 的任务头: Discussion of the task-specific head for token-level prediction, including the use of the `cls` component of the `AutoModelForMaskedLM` class, and the `BertOnlyMLMHead` class, which is used for token-level prediction.\n",
      " - 3.2. 范围输出的 Token 级预测: Explanation of the range output token-level prediction task, including the use of the `question-answering` pipeline and the `AutoModelForQuestionAnswering` class, which is used for question-answering and range output token-level prediction.\n",
      " - 3.3. 序列级分类头: Discussion of the sequence-level classification head, including the use of the `RobertaClassificationHead` class, which is used for sequence-level classification.\n",
      " - 3.4. 零样本分类: Explanation of zero-shot classification, including the use of the `zero-shot-classification` pipeline and the `facebook/bart-large-mnli` model, which is used for zero-shot classification.\n",
      " - Main Ideas and Relevance To Course: This notebook introduces the concept of task-specific heads and their use in language models, and explores the use of sequence-level classification and zero-shot classification in language modeling.\n",
      " - Important Code: `MyFillMaskModel`, `BertOnlyMLMHead`, `RobertaClassificationHead`, `AutoModelForMaskedLM`, `AutoModelForQuestionAnswering`, `pipeline('fill-mask')`, `pipeline('question-answering')`, `pipeline('zero-shot-classification')`, and `facebook/bart-large-mnli`.\n",
      " - Connections to previous notebooks: This notebook builds on the introduction to language models and pipelines in the previous notebook, and provides a deeper dive into the use of task-specific heads and sequence-level classification in language models.\n",
      " - Relevant Images: `imgs/task-token-classification.png`, `imgs/task-qa.png`, and `imgs/task-zero-shot.png`, showing the task-specific heads and the use of sequence-level classification and zero-shot classification in language models.\n",
      "\n",
      "This notebook is a crucial step in understanding the use of task-specific heads and sequence-level classification in language models, and provides a solid foundation for the rest of the course, which will explore more advanced topics in language modeling. The notebook introduces the concept of task-specific heads and their use in language models, and explores the use of sequence-level classification and zero-shot classification in language modeling. The connections to the previous notebook are strong, as this notebook builds on the introduction to language models and pipelines, and provides a deeper dive into the use of task-specific heads and sequence-level classification in language models.\n",
      "\n",
      "The next notebook will likely explore more advanced topics in language modeling, such as the use of transformer architectures for sequence-to-sequence tasks, and the use of attention mechanisms for more complex tasks. The notebook will likely introduce the concept of sequence-to-sequence models, and explore the use of attention mechanisms in these models. The notebook will also likely provide a deeper dive into the use of transformer architectures for sequence-to-sequence tasks, and introduce the concept of encoder-decoder architectures.\n",
      "************************************************************************************\n",
      "04_seq2seq.ipynb\n",
      " - 4.1. 机器翻译任务: Discussion of the machine translation task, including the use of encoder-decoder models, and the use of transformer architectures for sequence-to-sequence tasks.\n",
      " - 4.2. 引入 GPT 风格模型: Explanation of the GPT-style models, including the use of only a decoder, and the use of self-supervised learning for text generation.\n",
      " - 4.3. 编码器、解码器和编码-解码器: Discussion of the encoder, decoder, and encoder-decoder architectures, including the use of attention mechanisms and the concept of self-supervised learning.\n",
      " - 4.4. 使用 T5 风格的编码器-解码器进行机器翻译: Explanation of the use of T5-style encoder-decoder models for machine translation, including the use of a shared encoder and decoder, and the use of a task-specific head.\n",
      " - 4.5. 创建更通用的模型: Discussion of the creation of more general models, including the use of the Flan-T5 model, and the concept of in-context learning.\n",
      " - Main Ideas and Relevance To Course: This notebook introduces the concept of sequence-to-sequence models, and explores the use of encoder-decoder architectures and attention mechanisms in language modeling. It also introduces the concept of self-supervised learning and in-context learning.\n",
      " - Important Code: `pipeline('translation_en_to_fr')`, `pipeline('text2text-generation')`, `get_token_generator`, `transformers`, and `torch`.\n",
      " - Connections to previous notebooks: This notebook builds on the introduction to language models and pipelines in the previous notebook, and provides a deeper dive into the use of sequence-to-sequence models and encoder-decoder architectures in language modeling.\n",
      " - Relevant Images: `imgs/bert-vs-gpt.png`, `imgs/t5-architecture.png`, and `imgs/t5-pic.jpg`, showing the comparison between BERT and GPT, the T5 architecture, and the Flan-T5 model.\n",
      "\n",
      "This notebook is a crucial step in understanding the use of sequence-to-sequence models and encoder-decoder architectures in language modeling, and provides a solid foundation for the rest of the course, which will explore more advanced topics in language modeling. The notebook introduces the concept of sequence-to-sequence models, and explores the use of encoder-decoder architectures and attention mechanisms in language modeling. The connections to the previous notebook are strong, as this notebook builds on the introduction to language models and pipelines, and provides a deeper dive into the use of sequence-to-sequence models and encoder-decoder architectures in language modeling.\n",
      "\n",
      "The next notebook will likely explore more advanced topics in language modeling, such as the use of multi-modal generation, and the use of more complex architectures for sequence-to-sequence tasks. The notebook will likely introduce the concept of multi-modal generation, and explore the use of more complex architectures, such as the use of multiple encoders and decoders, and the use of attention mechanisms in these architectures. The notebook will also likely provide a deeper dive into the use of transformer architectures for sequence-to-sequence tasks, and introduce the concept of encoder-decoder architectures with multiple encoders and decoders.\n",
      "************************************************************************************\n",
      "05_multimodal.ipynb\n",
      " - 5.1: 定义模态？: Discussion of the concept of modality, including the definition of modality, and the different types of modalities, such as text, image, and audio.\n",
      " - 5.2: 编码不同的模态: Explanation of how to encode different modalities, including text, image, and audio, using transformer architectures.\n",
      " - 5.3: 联合投影（Joint Projection）: Discussion of joint projection, including the use of CLIP to project text and image embeddings into a shared space.\n",
      " - 5.4: 将多模态编码器与解码器结合: Explanation of how to combine multimodal encoders and decoders, including the use of cross-attention and transformer architectures.\n",
      " - 5.5: 扩散解码器简介: Introduction to diffusion decoders, including the use of policy networks and transformer architectures.\n",
      " - 5.6: 文本引导的图像扩散: Explanation of text-guided image diffusion, including the use of diffusion models and transformer architectures.\n",
      " - Main Ideas and Relevance To Course: This notebook introduces the concept of multimodal learning, and explores the use of transformer architectures for multimodal tasks, including text, image, and audio processing. It also introduces the concept of joint projection and diffusion decoders.\n",
      " - Important Code: `transformers`, `torch`, `BertTokenizer`, `BertModel`, `Wav2Vec2Tokenizer`, `Wav2Vec2Model`, `ViTImageProcessor`, `ViTModel`, `CLIPProcessor`, `CLIPModel`, `DiffusionPipeline`, and `DiffusionModel`.\n",
      " - Connections to previous notebooks: This notebook builds on the introduction to transformer architectures and sequence-to-sequence models in the previous notebooks, and provides a deeper dive into the use of multimodal learning and transformer architectures for multimodal tasks.\n",
      " - Relevant Images: `imgs/multimodal.png`, `imgs/wav2vec2.png`, `imgs/vit-model.png`, `imgs/clip-arch.png`, and `imgs/diffusion_img.png`, showing the comparison between different modalities, the architecture of Wav2Vec2, ViT, and CLIP, and the use of diffusion models for image generation.\n",
      "\n",
      "This notebook is a crucial step in understanding the use of multimodal learning and transformer architectures for multimodal tasks, and provides a solid foundation for the rest of the course, which will explore more advanced topics in multimodal learning and transformer architectures. The notebook introduces the concept of multimodal learning, and explores the use of transformer architectures for multimodal tasks, including text, image, and audio processing. The connections to the previous notebooks are strong, as this notebook builds on the introduction to transformer architectures and sequence-to-sequence models, and provides a deeper dive into the use of multimodal learning and transformer architectures for multimodal tasks.\n",
      "\n",
      "The next notebook will likely explore more advanced topics in multimodal learning, such as the use of more complex architectures for multimodal tasks, and the use of more advanced techniques, such as attention mechanisms and self-supervised learning. The notebook will likely introduce the concept of visual-linguistic models, and explore the use of more complex architectures, such as the use of multiple encoders and decoders, and the use of attention mechanisms in these architectures. The notebook will also likely provide a deeper dive into the use of transformer architectures for multimodal tasks, and introduce the concept of multimodal learning with multiple modalities.\n",
      "************************************************************************************\n",
      "06_llm_server.ipynb\n",
      " - 6.1: 将模型扩展到现实世界用例: Discussion of the limitations of basic generation models in production environments, including performance, efficiency, and scalability. Introduction to LLM (Large Language Model) services and their advantages.\n",
      " - 6.2: 访问您的第一个 LLM 服务: Introduction to vLLM (virtual LLM) and HuggingFace model services, including their features and benefits.\n",
      " - 6.3: 快速并发处理: Explanation of how to use vLLM for concurrent processing, including in-flight batching and asynchronous execution.\n",
      " - 6.4: 深入文本生成: Exploration of text generation capabilities using LLM services, including the use of LangChain for LLM orchestration.\n",
      " - Main Ideas and Relevance To Course: This notebook introduces the concept of LLM services and their advantages, and explores the use of vLLM for concurrent processing and text generation.\n",
      " - Important Code: `requests`, `OpenAI`, `langchain_nvidia_ai_endpoints`, `ChatNVIDIA`, `langchain_core`, `ChatPromptTemplate`, `StrOutputParser`, and `FileLister`.\n",
      " - Connections to previous notebooks: This notebook builds on the introduction to sequence-to-sequence models and transformer architectures in the previous notebooks, and provides a deeper dive into the use of LLM services and vLLM for concurrent processing and text generation.\n",
      " - Relevant Images: `imgs/llm-router.png`, `imgs/api-options.png`, and `imgs/basic-chat.png`, showing the architecture of LLM services, the options for accessing LLM models, and a basic chat interface.\n",
      "\n",
      "This notebook is a crucial step in understanding the use of LLM services and vLLM for concurrent processing and text generation, and provides a solid foundation for the rest of the course, which will explore more advanced topics in LLM services and vLLM. The notebook introduces the concept of LLM services and their advantages, and explores the use of vLLM for concurrent processing and text generation. The connections to the previous notebooks are strong, as this notebook builds on the introduction to sequence-to-sequence models and transformer architectures, and provides a deeper dive into the use of LLM services and vLLM for concurrent processing and text generation.\n",
      "\n",
      "The next notebook will likely explore more advanced topics in LLM services and vLLM, such as the use of more complex architectures for text generation, and the use of more advanced techniques, such as attention mechanisms and self-supervised learning. The notebook will likely introduce the concept of LLM orchestration using LangChain, and explore the use of more complex architectures, such as the use of multiple encoders and decoders, and the use of attention mechanisms in these architectures. The notebook will also likely provide a deeper dive into the use of vLLM for concurrent processing and text generation, and introduce the concept of multimodal learning with multiple modalities.\n",
      "************************************************************************************\n",
      "Here is the summary of the current notebook:\n",
      "\n",
      "07_intro_agentics.ipynb\n",
      " - 7.1: 构建 LLM 工作流: Discussion of the limitations of basic generation models in production environments, including performance, efficiency, and scalability. Introduction to LLM (Large Language Model) services and their advantages.\n",
      " - 7.2: 工具简介: Explanation of how to use LLM services for concurrent processing and text generation, including the use of LangChain for LLM orchestration.\n",
      " - 7.3: 多工具智能体系统: Introduction to the concept of LLM services and their advantages, and exploration of the use of vLLM for concurrent processing and text generation.\n",
      " - Main Ideas and Relevance To Course: This notebook introduces the concept of LLM services and their advantages, and explores the use of vLLM for concurrent processing and text generation.\n",
      " - Important Code: `requests`, `langchain_nvidia_ai_endpoints`, `ChatNVIDIA`, `langchain_core`, `ChatPromptTemplate`, `StrOutputParser`, and `FileLister`.\n",
      " - Connections to previous notebooks: This notebook builds on the introduction to sequence-to-sequence models and transformer architectures in the previous notebooks, and provides a deeper dive into the use of LLM services and vLLM for concurrent processing and text generation.\n",
      " - Relevant Images: `imgs/data-pipelines.png`, `imgs/simple-agent.png`, and `imgs/basic-react.png`, showing the architecture of LLM services, a simple agent, and a basic ReAct cycle.\n",
      "\n",
      "The current notebook is a crucial step in understanding the use of LLM services and vLLM for concurrent processing and text generation, and provides a solid foundation for the rest of the course, which will explore more advanced topics in LLM services and vLLM. The notebook introduces the concept of LLM services and their advantages, and explores the use of vLLM for concurrent processing and text generation. The connections to the previous notebooks are strong, as this notebook builds on the introduction to sequence-to-sequence models and transformer architectures, and provides a deeper dive into the use of LLM services and vLLM for concurrent processing and text generation.\n",
      "\n",
      "The next notebook will likely explore more advanced topics in LLM services and vLLM, such as the use of more complex architectures for text generation, and the use of more advanced techniques, such as attention mechanisms and self-supervised learning. The notebook will likely introduce the concept of LLM orchestration using LangChain, and explore the use of more complex architectures, such as the use of multiple encoders and decoders, and the use of attention mechanisms in these architectures. The notebook will also likely provide a deeper dive into the use of vLLM for concurrent processing and text generation, and introduce the concept of multimodal learning with multiple modalities.\n",
      "************************************************************************************\n",
      "08_assessment.ipynb\n",
      " - 8.1: 评估: Discussion of the evaluation process, including the use of LLM services for generating synthetic prompts, and the use of vision-language models for image generation.\n",
      " - 8.2: 运行评估: Explanation of how to run the evaluation process, including the use of LangChain for LLM orchestration and the submission of results to the assessment runner.\n",
      " - Main Ideas and Relevance To Course: This notebook introduces the concept of evaluation in the context of LLM services and vision-language models, and provides a practical example of how to use these concepts to generate synthetic prompts and images.\n",
      " - Important Code: `requests`, `langchain_nvidia_ai_endpoints`, `ChatNVIDIA`, `langchain_core`, `ChatPromptTemplate`, `StrOutputParser`, `DiffusionPipeline`, and `torch`.\n",
      " - Connections to previous notebooks: This notebook builds on the introduction to LLM services and vision-language models in the previous notebooks, and provides a practical example of how to use these concepts to generate synthetic prompts and images.\n",
      " - Relevant Images: `imgs/rad-assessment.png`, showing the architecture of the evaluation process.\n",
      "\n",
      "This notebook is a crucial step in understanding the use of LLM services and vision-language models for image generation and evaluation. The notebook provides a practical example of how to use these concepts to generate synthetic prompts and images, and introduces the concept of evaluation in the context of LLM services and vision-language models. The connections to the previous notebooks are strong, as this notebook builds on the introduction to LLM services and vision-language models, and provides a deeper dive into the use of these concepts for image generation and evaluation.\n",
      "\n",
      "The next notebook will likely explore more advanced topics in LLM services and vision-language models, such as the use of more complex architectures for image generation, and the use of more advanced techniques, such as attention mechanisms and self-supervised learning. The notebook will likely introduce the concept of multimodal learning with multiple modalities, and explore the use of more complex architectures, such as the use of multiple encoders and decoders, and the use of attention mechanisms in these architectures.\n",
      "************************************************************************************\n",
      "75_langgraph.ipynb\n",
      " - 7.5: LangGraph: Introduction to LangGraph, a popular multi-agent orchestration framework that makes some useful design decisions, serving as a great starting point for those interested in this field.\n",
      " - Main Ideas and Relevance To Course: LangGraph is introduced as a tool that allows for structured management of dialog flows using state graphs, enhancing extensibility and maintainability, especially in complex multi-agent systems or workflows.\n",
      " - Important Code: `langgraph`, `langchain_core`, `ConversationalToolCaller`, `ChatPromptTemplate`, `StrOutputParser`, `StateGraph`, `MemorySaver`, `RunnableConfig`, `TypedDict`, `Annotated`, `add_messages`, `tool`, `ToolNode`, `ToolMessage`.\n",
      " - Connections to previous notebooks: This notebook builds on the introduction to conversational AI and multi-agent systems in previous notebooks, and provides a practical example of how to use LangGraph to manage dialog flows and integrate with LLMs.\n",
      " - Relevant Images: None.\n",
      "\n",
      "This notebook is a crucial step in understanding the use of multi-agent orchestration frameworks like LangGraph, and how they can be used to manage complex dialog flows and integrate with LLMs. The notebook provides a practical example of how to use LangGraph to manage dialog flows and integrate with LLMs, and introduces the concept of state graphs and their use in managing complex dialog flows. The connections to the previous notebooks are strong, as this notebook builds on the introduction to conversational AI and multi-agent systems, and provides a deeper dive into the use of LangGraph for dialog flow management and LLM integration.\n",
      "\n",
      "The next notebook will likely explore more advanced topics in multi-agent systems and LLM integration, such as the use of more complex architectures for dialog flow management, and the use of more advanced techniques, such as attention mechanisms and self-supervised learning. The notebook will likely introduce the concept of multimodal learning with multiple modalities, and explore the use of more complex architectures, such as the use of multiple encoders and decoders, and the use of attention mechanisms in these architectures.\n",
      "************************************************************************************\n",
      "98_VLM_Launch.ipynb\n",
      " - 启动您的 vLLM 服务: This notebook introduces the use of vLLM (Visual Language Model) services, specifically the OpenAI style vLLM service, and provides a practical example of how to launch a vLLM service using the `vllm serve` command, with options such as `--trust-remote-code`, `--max-model-len`, `--gpu-memory-utilization`, and `--enforce-eager`.\n",
      " - Main Ideas and Relevance To Course: This notebook is a crucial step in understanding the use of vLLM services and their integration with LLMs, and provides a practical example of how to launch a vLLM service and use it with LLMs, building on the introduction to conversational AI and LLMs in previous notebooks.\n",
      " - Important Code: `vllm serve`, `--trust-remote-code`, `--max-model-len`, `--gpu-memory-utilization`, `--enforce-eager`, `microsoft/Phi-3.5-vision-instruct`.\n",
      " - Connections to previous notebooks: This notebook builds on the introduction to conversational AI and LLMs in previous notebooks, and provides a practical example of how to integrate vLLM services with LLMs, setting the stage for more advanced topics in multi-agent systems and LLM integration in future notebooks.\n",
      " - Relevant Images: None.\n",
      "************************************************************************************\n",
      "Table_of_Contents.ipynb\n",
      " - **Welcome to the course!**: Introduction to the course, overview of the structure and content, and a brief explanation of the microservices and caches used throughout the course.\n",
      " - **Microservices:** Overview of the microservices used in the course, including `./chatbot`, `./composer`, `./docker-router`, and `./llm_client`, with a brief description of their functions and purposes.\n",
      " - **Caches:** Overview of the caches used in the course, including `./nim-cache`, `./temp-dir`, `./imgs`, `./img-files`, `./audio-files`, and `./generated_images`, with a brief description of their functions and purposes.\n",
      "Main Ideas and Relevance To Course: This notebook provides an introduction to the course structure, microservices, and caches, setting the stage for the exploration of conversational AI and LLMs in subsequent notebooks, and providing a foundation for understanding the technical infrastructure used throughout the course.\n",
      "Important Code: `%%js`, `var url = 'http://'+window.location.host+':8990';`, `requests.get()`, `http://docker_router:8070/containers`, `http://docker_router:8070/containers/{service_name}/logs`.\n",
      "Connections to previous notebooks: This notebook does not build directly on previous notebooks, but provides a foundation for understanding the technical infrastructure used throughout the course, which will be built upon in subsequent notebooks.\n",
      "Relevant Images: None.\n",
      "************************************************************************************\n"
     ]
    }
   ],
   "source": [
    "message_prompt = (\n",
    "    \"Give me a structured summary of the course, making sure to note all sections and points?\"\n",
    "    \" This is a one-notebook-at-a-time process, and the next notebook is the one provided in context.\"\n",
    "    \"\\n\\nThe following is a running summary of previous notebooks: \\n\\n{running_summary}\\n\\n\"\n",
    "    \"Output only the summary of the currently-provided notebook, but explain the logical connections to the rest of the course.\"\n",
    "    \"Make sure the descriptions are also dense in key words that would be useful for searching through the notebooks via a bibliography.\"\n",
    "    \" Only output the following format, with no other structures, extra newlines, or info not grounded in context:\\n<notebook>.ipynb\"\n",
    "    \"\\n - <Section 1 Name (As Seen In Notebook)>: Decent Description, including frameworks used, important topics, etc.\"\n",
    "    \"\\n - ...\"\n",
    "    \"\\n - Main Ideas and Relevance To Course: Decent Description, dense with key features/frameworks/topics for bibliographic/semantic lookup.\"\n",
    "    \"\\n - Important Code: Types of syntaxes, variables, etc that are more specific to this notebook, like classes, variables, topics, terms, etc.\"\n",
    "    \"\\n - Connections to previous notebooks: Decent Description, dense with key features/frameworks/topics for bibliographic/semantic lookup.\"\n",
    "    \"\\n - Relevant Images: Brief descriptions of images (i.e. <img src='imgs/url.png'>, no non-image files) with local-scope URLs.\"\n",
    ")\n",
    "\n",
    "running_summaries = []\n",
    "running_summary = \"No summary yes. This is the first notebook.\"\n",
    "\n",
    "for name in filenames:\n",
    "    if not name.endswith(\".ipynb\"):\n",
    "        continue\n",
    "    buffer = \"\"\n",
    "    chat_state = {\n",
    "        \"filenames\": [name],\n",
    "        # \"messages\": [(\"human\", message_prompt.format(running_summary=running_summary))]                    ## Full history\n",
    "        \"messages\": [(\"human\", message_prompt.format(running_summary=(running_summaries or [\"None\"])[-1]))]  ## Sliding window\n",
    "        # \"messages\": [(\"human\", message_prompt.format(running_summary=\"Not provided\"))]                     ## No history, \n",
    "    }\n",
    "    for chunk in pipeline.stream(chat_state):\n",
    "        buffer += chunk\n",
    "        print(chunk, end=\"\")\n",
    "    print(\"\\n\" + \"*\" * 84)\n",
    "    running_summaries += [buffer]\n",
    "    running_summary = \"\\n\\n\".join(running_summaries)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7661dc41",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b>迭代优化的注意事项：</b></summary>\n",
    "<ul>\n",
    "    <li>上面的例子展示了几种不同的策略（全上下文、滑动窗口和无历史）。你会发现前两者生成的提炼（refinement）有所不同，但在当前这个上下文中还区别不大（不过全上下文在处理长文本提炼时会开始出现问题）。无历史也会有一些问题（有部分原因是需要更多信息，还有部分原因是提示词的指令暗示每个 notebook 都是第一个 notebook）。</li>\n",
    "    <li>注意对话和思维链（逐步思考）推理在本质上也是提炼问题，其响应是从全上下文（例如聊天历史）中提炼出来的，然后作为下一步提炼的新补充。</li>\n",
    "    <li>记住，LLM 在有“本可以实际生成”的输入时会受益匪浅。幻觉产生的一个常见原因是给 LLM 提供了它在没有充分告知的情况下不应能生成的上下文。</li>\n",
    "</ul>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c852a25",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### **并行生成**\n",
    "\n",
    "在某些情况下，迭代生成可能非常慢，或者可能累积到难以处理的上下文或错误。在这些情况下，您可能想要独立地推理输入的较小部分，稍后再将结果组合在一起。\n",
    "- **规范化：**如果您有一个主要由独立组件组成的任务，可以独立推理它们，并将其进展为标准化的表示，这样在后续处理时会更容易。例如，缩短文档的某些部分、提取重要细节，或者将简短的摘要扩展为更完整的形式。\n",
    "- **结构化：**在从长文档中提取全局上下文的任务中（即总结），您可以先对一个窗口进行摘要，然后再对这些摘要进行总结，依此类推，直到得到一个完整的摘要。类似的结构化思路包括[**将知识图谱插入值-边-值**](https://neo4j.com/developer-blog/knowledge-graph-llama-nvidia-langchain/)、[**将嵌入插入向量存储以进行语义检索**](https://arxiv.org/abs/2312.10997)、[**以及插入 SQL 数据库**](https://developer.nvidia.com/blog/new-llm-snowflake-arctic-model-for-sql-and-code-generation/)等，都可以在基于大型数据集应用 LLM 的大型应用中实现。\n",
    "- **集成：**给定 LLM 做某事的目标，可以使用不同方法的集成来独立尝试解决自然语言问题。这些方法的结果可以结合在一起形成最终的推理结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "357b06ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "from functools import partial\n",
    "import os\n",
    "filenames = [v for v in sorted(os.listdir(\"temp_dir\")) if v.endswith(\".ipynb\")]\n",
    "running_summaries = []\n",
    "message_prompt = (\n",
    "    \" Give me a rigorous summary of only bullet #{section_i} of the outline. Do not summarize any other sections.\"\n",
    "    \" Output should be a few compact-but-dense paragraphs long and only summarize a fraction of the notebook (bullet {section_i})\"\n",
    "    \" such that a reasonable person would be able to understand everything from that section\"\n",
    "    \" (while knowing roughly how it ties in with the whole notebook) from the summary.\" \n",
    ")\n",
    "\n",
    "notebook_chunks = []\n",
    "\n",
    "def summarize_section(state):\n",
    "    name, i = state.get(\"name\"), state.get(\"i\")\n",
    "    print(f\"(+{name[1]}.{i})\", end=\"\")\n",
    "    output = pipeline.invoke({\n",
    "        \"filenames\": [name],\n",
    "        \"messages\": [\n",
    "            (\"human\", \"Please give me a structured outline of the notebook\"), \n",
    "            (\"ai\", state.get(\"outline\")),\n",
    "            (\"human\", message_prompt.format(section_i=i))],\n",
    "    })\n",
    "    print(f\"(-{name[1]}.{i})\", end=\"\", flush=True)\n",
    "    return output\n",
    "\n",
    "task_keys = []\n",
    "task_args = []\n",
    "task_vals = []\n",
    "num_tasks = 6\n",
    "\n",
    "for name, outline in zip(filenames, running_summaries):\n",
    "    task_keys += (nb_keys := [f\"Notebook {name} Part {i}\" for i in range(1, num_tasks+1)])\n",
    "    task_args += (nb_args := [{\"name\": name, \"i\": i, \"outline\": outline} for i in range(1, num_tasks+1)])\n",
    "    ## Notebook-level parallelization: Bottlenecks down to one notebook at a time\n",
    "    task_vals += RunnableLambda(summarize_section).batch(nb_args)\n",
    "\n",
    "## All-at-once parallelization: Bottleneck is the set maximum concurrency (since threads are hardware-limited)\n",
    "task_vals = RunnableLambda(summarize_section).batch(task_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b69ea67",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b>并行化注意事项：</b></summary>\n",
    "\n",
    "- 如果您想在工作流的输入之间实现并行化，`batch` 是一个很好的选择，它已经包含了线程安全和并发限制（您可以通过观察第一个线程完成的时间来估计最大并发数）。如果您想在不同的链之间并行化（也就是说，您希望相同的数据同时通过多个管道），可以使用 `RunnableParallel`。这两者都比带信号量或线程池的方法更简单（但也更不灵活），后者允许更自定义化的处理方式。\n",
    "\n",
    "- 我们调整了提示词和数字，试图为每个部分生成摘要，但请注意，当 notebook 部分不足时，“总结第 i 部分”的目标是定义不清的。虽然这对我们在评估的目的来说已经足够，但在整体上并不是理想的。稍后您会学习一些策略来缓解这种“语言输入 -> 语言输出”的问题（***提示***：参见**结构化输出**）。\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c27dea",
   "metadata": {},
   "source": [
    "**保存结果**\n",
    "\n",
    "为了帮助可视化这些结果，您可以运行以下代码单元将其保存到 JSON 文件中。这些导出用于构建示例聊天机器人，因此看看这个工作流的输出会很有趣。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "22084679",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Note, we're going to need these generations for the assessment, so save them here\n",
    "short_summary = \"\"\n",
    "nbsummary = {\n",
    "    \"course\": \"NVIDIA Deep Learning Institute's Instructor-Led Course called \\\"Rapid Application Development with Large Language Models\\\"\",\n",
    "    \"summary\": short_summary, \n",
    "    \"filenames\": filenames\n",
    "}\n",
    "\n",
    "for i, (name, outline) in enumerate(zip(filenames, running_summaries)):\n",
    "    if not name.endswith(\".ipynb\"): continue\n",
    "    nbsummary[name] = {\"outline\": outline}\n",
    "    task_iter = range(i*num_tasks, (i+1)*num_tasks)\n",
    "    nbsummary[name][\"sections\"] = [task_vals[j] for j in task_iter]\n",
    "        \n",
    "import json\n",
    "json.dump(nbsummary, open('notebook_chunks.json', \"w\"), sort_keys=True, indent=4)\n",
    "\n",
    "with open('notebook_chunks.json', 'r') as fp:\n",
    "     nbsummary = json.load(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f1f8c5",
   "metadata": {},
   "source": [
    "#### **超越模型先验的进展**\n",
    "\n",
    "这些公式使大语言模型不仅能推理，还能生成任意长或任意短的响应。当以编程方式实现时，它们还使您可以控制上下文范围和瓶颈，在轻量但范围受限和信息充分但缓慢过程之间进行权衡。\n",
    "\n",
    "然而，这些技术本身只是建立在语言模型之上的工程，局限于其即时上下文、提示词和先验知识。**LLM 最大的潜力不在于模型权重和预计算工作流中的静态内容，而在于与丰富的环境结合，引导其推进并按照响应行动。**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d911c8c7",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<br>\n",
    "\n",
    "## **7.2：工具简介**\n",
    "\n",
    "**工具**通过将外部数据源、计算工具和动态系统整合到生成过程中，将这项技术提升到一个新水平。关键是，利用其内部参数以外的额外资源，可以进一步适应、检索、学习，从而在编排层面上影响环境。\n",
    "\n",
    "### **工具的基本原理：**\n",
    "\n",
    "为了使 LLM 能与外部环境交互，必须发生以下**一种**情况：\n",
    "- **观察系统：**其上下文可以通过一些程序化查询进行丰富，这些查询会根据应用状态而变化。\n",
    "    - **[可选]**在接收到依赖状态的上下文后，它能够改变应用状态。\n",
    "- **交互系统：**它必须能够向一个环境发送查询，该环境会响应其请求并改变应用状态。\n",
    "    - **[可选]**在请求后，它会从环境中收到反馈。\n",
    "\n",
    "**观察系统很容易理解，**因为唯一的要求是一些语义上可理解的上下文。用一些变量填充提示词，比如目录中的文件名，这样就有了一个能告诉您文件信息的 LLM 组件。允许用户选择要输入 LLM 的文档，这时它就是在与用户的选择进行推理。这里的难点不在于“如何以编程方式构建上下文”，而在于“放入什么”，难点是在选择上。\n",
    "\n",
    "**相比之下，交互系统的属性难以强制执行，**因为我们的 LLM 输出是非结构化的。上一部分中，我们的 LLM 之所以能与其它 LLM 交互，是因为它们都是文本输入和输出，但映射到普通非 LLM 工具就可能会面临挑战了，对吧？"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4edfe160",
   "metadata": {},
   "source": [
    "\n",
    "#### **语法强制（Grammar Enforcement）**\n",
    "\n",
    "幸运的是，LLM 采样工具和编排工具在努力弥合这一鸿沟，允许我们通过一种叫做**语法强制**的方式直接调用工具！\n",
    "\n",
    "回想一下我们在早期部分讨论的工作流，算法输入模式自然定义为键值对字典：\n",
    "\n",
    "```python\n",
    "{\n",
    "    \"arg1\": value1,\n",
    "    \"arg2\": value2,\n",
    "    ...\n",
    "}\n",
    "```\n",
    "\n",
    "从 LLM 的角度来看，假设变量名和值是人类可理解的，就可以毫无障碍地将其作为上下文输入。然而，生成这样的模式就没那么简单了。开发者们早早意识到，虽然您可以“请求”一个 LLM 生成“有效的 python 代码”或“仅包含正确键的有效 JSON”，但这些策略通常需要后处理，并且失败的情况会时常出现。\n",
    "\n",
    "如今，许多模型支持**“模式”**输入，指定输出所需的格式。考虑以下工具实例化，它创建了一个自然适配该接口的对象："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5374842c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "add\n",
      "Adds a and b. Requires both arguments. Can be repurposed for subtraction\n",
      "{'explanation_of_what_the_user_wants': {'title': 'Explanation Of What The User Wants', 'type': 'string'}, 'a': {'title': 'A', 'type': 'number'}, 'b': {'title': 'B', 'type': 'number'}}\n"
     ]
    }
   ],
   "source": [
    "from langchain.tools import tool\n",
    "\n",
    "@tool\n",
    "def add(\n",
    "    explanation_of_what_the_user_wants: str, ## Optional. Gives some food for thought.\n",
    "    a: float, \n",
    "    b: float\n",
    ") -> int:\n",
    "    \"\"\"Adds a and b. Requires both arguments. Can be repurposed for subtraction\"\"\"\n",
    "    return a + b\n",
    "\n",
    "print(add.name)\n",
    "print(add.description)\n",
    "print(add.args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b87f3618",
   "metadata": {},
   "source": [
    "当输入模式给到 LLM 服务时，生成必须在采样层面上遵循所需的语法。换句话说，无论从每次自回归调用生成的整体概率向量如何：\n",
    "- 前几个 token 必须是 `{'a': `，按此顺序。\n",
    "- token 范围将限制在有效 token 的某个子集 `0123456789.e-+}` 之内，直到生成 `}`。\n",
    "- 接下来的 token 采样必须是 `}, {'b': `。\n",
    "- 一直重复，最后的 `}` 是强制停止 token。\n",
    "\n",
    "鉴于我们的 `tool` 组件在构建过程中会自动聚合输入模式等细节，我们可以简单地使用 `with_structured_output` 将输入模式绑定到连接器，然后假设其调用必须遵循。\n",
    "\n",
    "下面是一个示例，附带一些重要的注意事项：\n",
    "\n",
    "- LLM 本身并不知道这种语法强制，若不加以控制可能会出错。因此如果没有更一般的格式遵循指令，比如默认的 LangChain 指令格式字符串，那么给 LLM 提供参数模式是个好习惯。\n",
    "\n",
    "- 除了语法强制，Llama 和其它类似模型[**明确以支持函数调用为目标进行训练**](https://github.com/meta-llama/llama-models/blob/6ad6fd6bb8f5fc841acecc2e48958eee25ff3b1c/models/llama3_1/prompt_format.md?plain=1#L306)。您会在后面的部分注意到，语法强制可能和 `<function=foo>{}</function>` 这样的输出配合使用一个解析器。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f90da0a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PASSED] 299079545.6498718 + 13990229401.41057 = 14289308947.060442 vs 14289308947.060442\n",
      "\tThoughts: Add the values of 299079545.6498718 and 13990229401.41057\n",
      "[PASSED] 49972403607.08792 + 13836264311.261892 = 63808667918.349815 vs 63808667918.349815\n",
      "\tThoughts: Add the values of 49972403607.08792 and 13836264311.261892\n",
      "[PASSED] 66135839409.68876 + 12812400332.099615 = 78948239741.78838 vs 78948239741.78838\n",
      "\tThoughts: Add the values of 66135839409.68876 and 12812400332.099615\n",
      "[PASSED] 16419345503.16172 + 31371985175.724094 = 47791330678.88582 vs 47791330678.88582\n",
      "\tThoughts: Add the values of 16419345503.16172 and 31371985175.724094\n",
      "[PASSED] 90948212999.80411 + 23939645009.650444 = 114887858009.45456 vs 114887858009.45456\n",
      "\tThoughts: Add the values of 90948212999.80411 and 23939645009.650444\n",
      "[PASSED] 35930890594.010315 + 78944447083.26022 = 114875337677.27054 vs 114875337677.27054\n",
      "\tThoughts: Add the values of 35930890594.010315 and 78944447083.26022\n",
      "[PASSED] 3445549933.2008753 + 32112080101.36588 = 35557630034.56676 vs 35557630034.56676\n",
      "\tThoughts: Add the values of 3445549933.2008753 and 32112080101.36588\n",
      "[PASSED] 36229971818.982475 + 15272198053.718178 = 51502169872.70065 vs 51502169872.70065\n",
      "\tThoughts: Add the values of 36229971818.982475 and 15272198053.718178\n",
      "[PASSED] 57772024874.28047 + 9705765657.674303 = 67477790531.95477 vs 67477790531.95477\n",
      "\tThoughts: Add the values of 57772024874.28047 and 9705765657.674303\n",
      "[PASSED] 14623810811.60894 + 64356960348.716415 = 78980771160.32535 vs 78980771160.32535\n",
      "\tThoughts: Add the values of 14623810811.60894 and 64356960348.716415\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import PydanticOutputParser, StrOutputParser\n",
    "from random import random\n",
    "\n",
    "add_tool = (\n",
    "    llm.with_structured_output(add.input_schema).bind(temperature=0)\n",
    "    | dict\n",
    "    | {\n",
    "        \"args\": RunnablePassthrough(),\n",
    "        \"result\": add,\n",
    "    }\n",
    ")\n",
    "\n",
    "for i in range(10):\n",
    "\n",
    "    a, b = random() * 10e10, random() * 10e10 \n",
    "    # a, b = round(a), round(b)\n",
    "    \n",
    "    tool_output = add_tool.invoke([\n",
    "        (\"system\", PydanticOutputParser(pydantic_object=add.input_schema).get_format_instructions()),\n",
    "        # (\"system\", f\"Assume schema of {add.args}\"),  ## Lighter reinforcement.\n",
    "        (\"user\", f\"Add the values of {a} and {b}.\"),\n",
    "    ])\n",
    "\n",
    "    tool_a = tool_output.get(\"args\").get(\"a\")\n",
    "    tool_b = tool_output.get(\"args\").get(\"b\")\n",
    "    tool_result = tool_output.get(\"result\")\n",
    "    \n",
    "    print(\"[PASSED]\" if (a+b) == tool_result else \"[FAILED]\", end=\" \")\n",
    "    print(f\"{tool_a} + {tool_b} = {tool_result} vs {a + b}\" )\n",
    "    \n",
    "    if \"explanation_of_what_the_user_wants\" in tool_output.get(\"args\"):\n",
    "        print(\"\\tThoughts:\", tool_output.get(\"args\").get(\"explanation_of_what_the_user_wants\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f5995c",
   "metadata": {},
   "source": [
    "**试试:**\n",
    "- 如果您不提供系统消息，会发生什么？它还会有效吗？\n",
    "- 如果您将系统消息移回用户消息中，会发生什么？它也能正常工作吗？\n",
    "- 如果您不包含 `explanation_of_what_the_user_wants` 变量，会发生什么？效果是更好还是更差？\n",
    "\n",
    "从微观层面上看，您让 LLM 做了加法！这其实……说实话也没什么大不了的，对吧？好吧，就算在这个狭义的范围内，这可能也比让 LLM 为您做这件事要好……"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8057f2bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[FAILED] 47348038448 + 36809992442 = 84258030890 vs 84158030890\n",
      "[FAILED] 18140873464 + 71272568143 = 19911141607 vs 89413441607\n",
      "[FAILED] 9770303212 + 33523382949 = 33523382949 + 9770303212 = 43393686161 vs 43293686161\n",
      "[FAILED] 61645989790 + 26289043188 = 86835032978 vs 87935032978\n",
      "[FAILED] 63225047461 + 70862947766 = 123887952127 vs 134087995227\n",
      "[FAILED] 47200712415 + 52922290442 = 12322992857 vs 100123002857\n",
      "[FAILED] 90718670404 + 99554682394 = 211733528098 vs 190273352798\n",
      "[FAILED] 14401877524 + 64896804768 = 12398782292 vs 79298682292\n",
      "[FAILED] 77953819053 + 62976947603 = 108030766556 vs 140930766656\n",
      "[PASSED] 16974252632 + 68452494489 = 16974252632 + 68452494489 = 85426747121 vs 85426747121\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "\n",
    "    a, b = random() * 10e10, random() * 10e10 \n",
    "    a, b = round(a), round(b)\n",
    "    \n",
    "    tool_output = (llm | StrOutputParser()).invoke([\n",
    "        (\"user\", f\"Add the values of {a} and {b}. Only return the final answer, not the arithmetic\"),\n",
    "    ])\n",
    "    \n",
    "    print(\"[PASSED]\" if str(a+b) in tool_output else \"[FAILED]\", end=\" \")\n",
    "    print(f\"{a} + {b} = {tool_output} vs {a + b}\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b68dfd67",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "但从宏观层面来看，我们可以扩展结构化输出生成，让我们的 LLM 遵循几乎任何预定义的输出模式。这项能力对于将 LLM 视为更大软件工作流的核心组件至关重要。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a3d5d5",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<br>\n",
    "\n",
    "## **7.3：** 多工具智能体系统\n",
    "\n",
    "我们之前提到，和环境交互需要一个系统，这个系统要么能观察并推理动态环境，要么能直接影响并对其做出反应。能够同时做到这两点的系统被称为**智能体**。\n",
    "\n",
    "> 更一般地说，如果它能够观察、思考、反应，并根据个人指令对环境采取行动，。\n",
    ">\n",
    "> 换句话说，如果它能根据当前状态选择工具或行动，利用它影响某些事物，并理解其决策如何推动其朝着目标前进，这个系统就是***智能体***。\n",
    "\n",
    "**至少，智能体通常只需要一个组件：** \n",
    "- 一个选择路径的路由机制。\n",
    "\n",
    "**除此之外，许多智能体还包括：**\n",
    "- 对所选路径参数的预测（如果需要）。\n",
    "- 一个用来积累记忆的缓冲区（如果需要）。\n",
    "- 一个总体或选择性应用的指令（如果需要）。\n",
    "\n",
    "<div><img src=\"imgs/simple-agent.png\" width=\"600\"/></div>\n",
    "\n",
    "<br>\n",
    "\n",
    "按照这个定义，下面的示例在技术上是一个基本的智能体循环，至少满足作为智能体的要求："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "be9b8142",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "[Human]: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Agent]: [pass]KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableBranch\n",
    "from random import random\n",
    "\n",
    "sys_msg = (\n",
    "    \"Please help the user. After every response, output '[stop]` if the conversation should end and [pass] otherwise.\"\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([(\"system\", sys_msg), (\"placeholder\", \"{messages}\")])\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "state = {\"messages\": []}\n",
    "agent_msg = \"\"\n",
    "\n",
    "while True:\n",
    "    try: \n",
    "        if \"[stop]\" in agent_msg: break\n",
    "        else: pass\n",
    "        \n",
    "        ## TODO: Update the messages appropriately\n",
    "        human_msg = input(\"\\n[Human]:\")\n",
    "        state[\"messages\"] += [(\"human\", human_msg)]\n",
    "\n",
    "        ## Initiate an agent buffer to accumulate agent response\n",
    "        agent_msg = \"\"\n",
    "        print(\"\\n[Agent]: \", end=\"\")\n",
    "        ## TODO: Stream the LLM's response directly to output and accumulate it\n",
    "        for token in chain.stream(state):\n",
    "            agent_msg += token\n",
    "            print(token, end=\"\")\n",
    "\n",
    "        ## TODO: Update the messages list appropriately\n",
    "        state[\"messages\"] += [(\"ai\", agent_msg)]\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"KeyboardInterrupt\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d4e0771",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "在我们的智能体抽象中，上面的循环使用了一个简单的*“输出是否包含 `[stop]`”*的启发式来决定是否继续对话。这算不算微不足道？是的，从技术上讲，这只是一个路由机制 `\"[stop]\" in agent_msg` 和随后的工具调用（`break`），但其逻辑延伸却出乎意料地强大！\n",
    "\n",
    "- **[多工具]**如果我们有比 `pass` 和 `break` 更多的工具会怎样？\n",
    "- **[状态管理]**如果我们的工具选择（以及工具的参数选择）改变了系统的行为呢？\n",
    "- **[检索]**如果我们的工具为上下文提供了相关信息呢？\n",
    "- **[智能多模态]**如果我们的工具允许智能体在必要时输出其它模态（图像、音频、视频等）呢？\n",
    "\n",
    "### **一个简单的多工具智能体**\n",
    "\n",
    "在上面提到的各个类中，**多工具智能体**是最包罗万象的，因为几乎所有的例子*都*可以归类于此。为了探索这个概念，让我们创建一个具有以下工具的计算器智能体："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1d0964d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "\n",
    "@tool\n",
    "def add(a: float, b: float) -> int:\n",
    "    \"\"\"Adds a and b. Requires both arguments.\"\"\"\n",
    "    return a + b\n",
    "\n",
    "@tool\n",
    "def subtract(a: float, b: float) -> int:\n",
    "    \"\"\"Subtracts a and b. Requires both arguments.\"\"\"\n",
    "    return a + b\n",
    "\n",
    "@tool\n",
    "def multiply(a: float, b: float) -> int:\n",
    "    \"\"\"Multiplies a and b. Requires both arguments.\"\"\"\n",
    "    return a * b\n",
    "\n",
    "@tool\n",
    "def divide(a: float, b: float) -> int:\n",
    "    \"\"\"Divides a by b. Requires both arguments.\"\"\"\n",
    "    return a / b\n",
    "\n",
    "@tool\n",
    "def power(a: float, b: float) -> int:\n",
    "    \"\"\"Raises a to the power of b. Requires both arguments.\"\"\"\n",
    "    return a ** b\n",
    "\n",
    "@tool\n",
    "def no_tool() -> str:\n",
    "    \"\"\"Null tool; says no tool should be used\"\"\"\n",
    "    return \"No Tool Selected\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c9643a",
   "metadata": {},
   "source": [
    "如果我们想的话，可以创建一个路由机制，首先预测使用哪个工具，然后调用该工具。然而，这个过程在许多系统中是如此常见且有用，以至于很多系统在服务端以类似于结构化输出的方式支持它：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "63497d1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'chatcmpl-tool-addd88be9b9a46d28b274867e6990d50', 'type': 'function', 'function': {'name': 'multiply', 'arguments': '{\"a\": 56766, \"b\": 30432}'}}]}, response_metadata={'role': 'assistant', 'content': None, 'tool_calls': [{'id': 'chatcmpl-tool-addd88be9b9a46d28b274867e6990d50', 'type': 'function', 'function': {'name': 'multiply', 'arguments': '{\"a\": 56766, \"b\": 30432}'}}], 'token_usage': {'prompt_tokens': 547, 'total_tokens': 575, 'completion_tokens': 28}, 'finish_reason': 'tool_calls', 'model_name': 'meta/llama-3.1-8b-instruct'}, id='run-1097d7b5-6a56-47e8-90d4-3fb685668b91-0', tool_calls=[{'name': 'multiply', 'args': {'a': 56766, 'b': 30432}, 'id': 'chatcmpl-tool-addd88be9b9a46d28b274867e6990d50', 'type': 'tool_call'}], role='assistant')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.runnables import Runnable, RunnableAssign, RunnablePassthrough, RunnableLambda\n",
    "from langgraph.prebuilt import ToolNode\n",
    "\n",
    "\n",
    "math_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You're a math bot! Help the user as much as possible.\"),\n",
    "    (\"placeholder\", \"{messages}\"),\n",
    "])\n",
    "\n",
    "toolset = [\n",
    "    add,\n",
    "    multiply,\n",
    "    divide,\n",
    "    power,\n",
    "    no_tool,\n",
    "]\n",
    "\n",
    "## Create a client-side resolved which executes the tool picked by the server.\n",
    "tool_node = ToolNode(toolset)\n",
    "\n",
    "simple_chain = (\n",
    "    math_prompt \n",
    "    ## Bind the tools to the connector, effectively feeding in the possible schemas on every invocation.\n",
    "    | llm.bind_tools(toolset)\n",
    "    # | {\"messages\": lambda x: [x]} | tool_node | RunnableLambda(lambda x: x.get(\"messages\"))\n",
    ")\n",
    "simple_chain.invoke({\"messages\": [(\"user\", \"What's 56766*30432?\")]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "86e4d07c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Simple question = \"What's 333333*555555?\"\n",
      "content='' additional_kwargs={'tool_calls': [{'id': 'chatcmpl-tool-3473dea047d44fe8bc1baff394775be8', 'type': 'function', 'function': {'name': 'multiply', 'arguments': '{\"a\": 333333, \"b\": 555555}'}}]} response_metadata={'role': 'assistant', 'content': None, 'tool_calls': [{'id': 'chatcmpl-tool-3473dea047d44fe8bc1baff394775be8', 'type': 'function', 'function': {'name': 'multiply', 'arguments': '{\"a\": 333333, \"b\": 555555}'}}], 'token_usage': {'prompt_tokens': 547, 'total_tokens': 575, 'completion_tokens': 28}, 'finish_reason': 'tool_calls', 'model_name': 'meta/llama-3.1-8b-instruct'} id='run-b1ca88eb-0f2e-4570-8260-aa9428e02261-0' tool_calls=[{'name': 'multiply', 'args': {'a': 333333, 'b': 555555}, 'id': 'chatcmpl-tool-3473dea047d44fe8bc1baff394775be8', 'type': 'tool_call'}] role='assistant'\n",
      "\n",
      "Double question = \"What's 333333*555555 and 444444+222222?\"\n",
      "content='' additional_kwargs={'tool_calls': [{'id': 'chatcmpl-tool-a1e621b45b5c457d84e65263bf8b7c7c', 'type': 'function', 'function': {'name': 'multiply', 'arguments': '{\"a\": 333333, \"b\": 555555}'}}]} response_metadata={'role': 'assistant', 'content': None, 'tool_calls': [{'id': 'chatcmpl-tool-a1e621b45b5c457d84e65263bf8b7c7c', 'type': 'function', 'function': {'name': 'multiply', 'arguments': '{\"a\": 333333, \"b\": 555555}'}}], 'token_usage': {'prompt_tokens': 554, 'total_tokens': 589, 'completion_tokens': 35}, 'finish_reason': 'tool_calls', 'model_name': 'meta/llama-3.1-8b-instruct'} id='run-70e41598-574c-4fc8-85ba-9214ed1eb4a3-0' tool_calls=[{'name': 'multiply', 'args': {'a': 333333, 'b': 555555}, 'id': 'chatcmpl-tool-a1e621b45b5c457d84e65263bf8b7c7c', 'type': 'tool_call'}] role='assistant'\n",
      "\n",
      "Complex question = \"What's 555555 times (444444 plus 222222)?\"\n",
      "content='' additional_kwargs={'tool_calls': [{'id': 'chatcmpl-tool-e3043423b6ff4905bb2c9b582609a920', 'type': 'function', 'function': {'name': 'multiply', 'arguments': '{\"a\": 555555, \"b\": 444444}'}}]} response_metadata={'role': 'assistant', 'content': None, 'tool_calls': [{'id': 'chatcmpl-tool-e3043423b6ff4905bb2c9b582609a920', 'type': 'function', 'function': {'name': 'multiply', 'arguments': '{\"a\": 555555, \"b\": 444444}'}}], 'token_usage': {'prompt_tokens': 552, 'total_tokens': 588, 'completion_tokens': 36}, 'finish_reason': 'tool_calls', 'model_name': 'meta/llama-3.1-8b-instruct'} id='run-b9034889-1a4e-4ae6-86be-0e440b47b118-0' tool_calls=[{'name': 'multiply', 'args': {'a': 555555, 'b': 444444}, 'id': 'chatcmpl-tool-e3043423b6ff4905bb2c9b582609a920', 'type': 'tool_call'}] role='assistant'\n",
      "\n",
      "Random question = 'Introduce yourself in 10 words or less!'\n",
      "content=\"I'm a math bot here to assist you.\" response_metadata={'role': 'assistant', 'content': \"I'm a math bot here to assist you.\", 'token_usage': {'prompt_tokens': 548, 'total_tokens': 559, 'completion_tokens': 11}, 'finish_reason': 'stop', 'model_name': 'meta/llama-3.1-8b-instruct'} id='run-2f3df61d-2393-4be7-819d-47edd3fa2dc2-0' role='assistant'\n"
     ]
    }
   ],
   "source": [
    "question = \"What's 333333*555555?\"\n",
    "print(f\"\\nSimple {question = }\")\n",
    "print(simple_chain.invoke({\"messages\": [(\"user\", question)]}))\n",
    "\n",
    "question = \"What's 333333*555555 and 444444+222222?\"\n",
    "print(f\"\\nDouble {question = }\")\n",
    "print(simple_chain.invoke({\"messages\": [(\"user\", question)]}))\n",
    "\n",
    "question = \"What's 555555 times (444444 plus 222222)?\"\n",
    "print(f\"\\nComplex {question = }\")\n",
    "print(simple_chain.invoke({\"messages\": [(\"user\", question)]}))\n",
    "\n",
    "question = \"Introduce yourself in 10 words or less!\"\n",
    "print(f\"\\nRandom {question = }\")\n",
    "print(simple_chain.invoke({\"messages\": [(\"user\", question)]}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bca957db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'url': 'http://nim:8000/v1/chat/completions',\n",
       " 'headers': {'Accept': 'application/json',\n",
       "  'Authorization': 'Bearer **********',\n",
       "  'User-Agent': 'langchain-nvidia-ai-endpoints'},\n",
       " 'json': {'messages': [{'role': 'system',\n",
       "    'content': \"You're a math bot! Help the user as much as possible.\"},\n",
       "   {'role': 'user', 'content': 'Introduce yourself in 10 words or less!'}],\n",
       "  'model': 'meta/llama-3.1-8b-instruct',\n",
       "  'temperature': 0.0,\n",
       "  'max_tokens': 5000,\n",
       "  'stream': False,\n",
       "  'tools': [{'type': 'function',\n",
       "    'function': {'name': 'add',\n",
       "     'description': 'Adds a and b. Requires both arguments.',\n",
       "     'parameters': {'type': 'object',\n",
       "      'properties': {'a': {'type': 'number'}, 'b': {'type': 'number'}},\n",
       "      'required': ['a', 'b']}}},\n",
       "   {'type': 'function',\n",
       "    'function': {'name': 'multiply',\n",
       "     'description': 'Multiplies a and b. Requires both arguments.',\n",
       "     'parameters': {'type': 'object',\n",
       "      'properties': {'a': {'type': 'number'}, 'b': {'type': 'number'}},\n",
       "      'required': ['a', 'b']}}},\n",
       "   {'type': 'function',\n",
       "    'function': {'name': 'divide',\n",
       "     'description': 'Divides a by b. Requires both arguments.',\n",
       "     'parameters': {'type': 'object',\n",
       "      'properties': {'a': {'type': 'number'}, 'b': {'type': 'number'}},\n",
       "      'required': ['a', 'b']}}},\n",
       "   {'type': 'function',\n",
       "    'function': {'name': 'power',\n",
       "     'description': 'Raises a to the power of b. Requires both arguments.',\n",
       "     'parameters': {'type': 'object',\n",
       "      'properties': {'a': {'type': 'number'}, 'b': {'type': 'number'}},\n",
       "      'required': ['a', 'b']}}},\n",
       "   {'type': 'function',\n",
       "    'function': {'name': 'no_tool',\n",
       "     'description': 'Null tool; says no tool should be used',\n",
       "     'parameters': {'type': 'object', 'properties': {}}}}]}}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm._client.last_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d59d4094",
   "metadata": {},
   "source": [
    "**从这个演示中可以看到：**\n",
    "- 这个服务部署中的内部路由器一次只能够思考一个工具调用。\n",
    "- 这个内部路由实际上并没有向模型通报其模式。它只是在生成时强制执行这些模式。\n",
    "- 您可能偶尔会看到生成内容而不是 `no_tool` 调用。这意味着除了客户端的 no_tool 支持外，还有一个服务端版本，它改变了端点对于非结构化生成的行为。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d9b749",
   "metadata": {},
   "source": [
    "### **[进阶]** 会话工具调用\n",
    "\n",
    "通过一些更先进的 LLM 编排范式，我们可以在默认的工具调用实现上进行改进，创建一个能够在看似单一的生成中同时进行**会话和工具调用**服务。这是一个较为复杂的话题，我们不会详细讨论，但提到它有几个重要原因：\n",
    "- 与服务端智能能力相关的直觉支撑了许多更高级的自定义功能，如工具感知的端点、服务端反思/思维链，以及特定入口的知识库。\n",
    "- 在评估中，我们希望利用一个更简化的智能框架，称为 LangGraph，它使得会话工具调用变得更简单。通过构建和激励这个模块，我们将能够与像 OpenAI 的 GPT4 这样的模型保持代码的一致性。\n",
    "\n",
    "以下代码块介绍了一个自定义的 `ConversationalToolCaller` 组件，旨在说明如何将一个定制的工具调用方式转变为一个简化的入口："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ba580b45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AIMessage(content='To calculate this expression, I\\'ll use the tools to break it down step by step.\\n\\nFirst, I\\'ll multiply 56766 and 30432:\\n\\n<function=\"multiply\">[\"a\": 56766, \"b\": 30432]</function>', additional_kwargs={'tool_calls': [{'id': 'chatcmpl-tool-e67ca132a082432f9f8625aa601339d6', 'type': 'function', 'function': {'name': 'multiply', 'arguments': '{\"a\": 56766, \"b\": 30432}'}}]}, response_metadata={'finish_reason': 'stoptool_calls', 'model_name': 'meta/llama-3.1-8b-instructmeta/llama-3.1-8b-instruct', 'role': 'assistant', 'content': None, 'tool_calls': [{'id': 'chatcmpl-tool-e67ca132a082432f9f8625aa601339d6', 'type': 'function', 'function': {'name': 'multiply', 'arguments': '{\"a\": 56766, \"b\": 30432}'}}], 'token_usage': {'prompt_tokens': 1158, 'total_tokens': 1186, 'completion_tokens': 28}}, id='run-1b1f7e29-ee87-4526-a1ac-f91e77d80871', tool_calls=[{'name': 'multiply', 'args': {'a': 56766, 'b': 30432}, 'id': 'chatcmpl-tool-e67ca132a082432f9f8625aa601339d6', 'type': 'tool_call'}])\n"
     ]
    }
   ],
   "source": [
    "from chatbot.conv_tool_caller import ConversationalToolCaller\n",
    "\n",
    "agent_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", (\n",
    "        \"You're a math bot! Help the user as much as possible by using the tools to answer their questions.\"\n",
    "        \" Think step-by-step, and work out your math using the tools provided.\"\n",
    "    )),\n",
    "    (\"placeholder\", \"{messages}\"),\n",
    "])\n",
    "\n",
    "tool_instruction = (\n",
    "    \"You have access to the tools listed in the toolbank. Use tools only within the \\n<function></function> tags.\"\n",
    "    \" Select tools to handle uncertain, imprecise, or complex computations that an LLM would find it hard to answer.\"\n",
    "    \" You can only call one tool at a time, and the tool cannot accept complex multi-step inputs.\"\n",
    "    \"\\n\\n<toolbank>{toolbank}</toolbank>\\n\"\n",
    "    \"Examples (WITH HYPOTHETICAL TOOLS):\"\n",
    "    \"\\nSure, let me call the tool in question.\\n<function=\\\"foo\\\">[\\\"input\\\": \\\"hello world\\\"]</function>\"\n",
    "    \"\\nSure, first, I need to calculate the expression of 5 + 10\\n<function=\\\"calculator\\\">[\\\"expression\\\": \\\"5 + 10\\\"]</function>\"\n",
    "    \"\\nSure! Let me look up the weather in Tokyo\\n<function=\\\"weather\\\">[\\\"location\\\"=\\\"Tokyo\\\"])</function>\"\n",
    ")\n",
    "\n",
    "tool_prompt = (\n",
    "    \"You are an expert at selecting tools to answer questions. Consider the context of the problem,\"\n",
    "    \" what has already been solved, and what the immediate next step to solve the problem should be.\"\n",
    "    \" Do not predict any arguments which are not present in the context; if there's any ambiguity, use no_tool.\"\n",
    "    \"\\n\\n<toolbank>{toolbank}</toolbank>\\n\"\n",
    "    \"\\n\\nSchema Instructions: The output should be formatted as a JSON instance that conforms to the JSON schema.\"\n",
    "    \"\\n\\nExamples (WITH HYPOTHETICAL TOOLS):\"\n",
    "    \"\\n<function=\\\"search\\\">[\\\"query\\\": \\\"current events in Japan\\\"]</function>\"\n",
    "    \"\\n<function=\\\"translation\\\">[\\\"text\\\": \\\"Hello, how are you?\\\", \\\"language\\\": \\\"French\\\"]</function>\"\n",
    "    \"\\n<function=\\\"calculator\\\">[\\\"expression\\\": \\\"5 + 10\\\"]</function>\"\n",
    ")\n",
    "\n",
    "conv_llm = ConversationalToolCaller(\n",
    "    tool_instruction=tool_instruction, \n",
    "    tool_prompt=tool_prompt, \n",
    "    llm=llm\n",
    ").get_tooled_chain()\n",
    "\n",
    "agent_chain = agent_prompt | conv_llm.bind_tools(toolset)\n",
    "\n",
    "response1 = agent_chain.invoke({\"messages\": [(\"user\", \"What's (56766*30432+3043)/99?\")]})\n",
    "print(repr(response1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae38f76",
   "metadata": {},
   "source": [
    "如果是流式传输，上面的代码将首先与消息相关联生成一个 token，然后在保持语法约束的同时转储工具调用的函数参数。当不是流式传输时，响应将按预期一次性返回。\n",
    "\n",
    "在这个调用之后，常见的做法是使用 ToolNode 组件来去除消息中的工具调用，并将每个工具调用与合适的执行器配对，以产生正确的输出。可以按如下方式完成："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0a4387f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ToolMessage(content='1727502912.0', name='multiply', tool_call_id='chatcmpl-tool-e67ca132a082432f9f8625aa601339d6')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langgraph.prebuilt import ToolNode\n",
    "\n",
    "tool_caller = RunnableLambda(lambda string: ToolNode(toolset).invoke({\"messages\": [string]})[\"messages\"])\n",
    "\n",
    "tool_caller.invoke(response1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a873e5de",
   "metadata": {},
   "source": [
    "这段代码可在 [`conv_tool_caller.py`](conv_tool_caller.py) 中找到，详细的拆解超出了课程的范围。然而，我们仍推荐您花时间看看，因为其中有关于通过入口传输流和接受客户端访问的自定义参数的不错的信息。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f279dfc",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<br>\n",
    "\n",
    "## **[练习] 7.4：启用智能循环**\n",
    "\n",
    "现在我们有了一个会话工具调用入口，接下来就可以创建一个类似**智能循环**的东西了。\n",
    "- 在之前的 notebook 中，我们设定了一个**简单的聊天循环**，可以让我们积累聊天记录。\n",
    "- 在这个 notebook 中，我们将设置一个**多步骤智能循环**，允许我们的系统在给用户返回最终答案前调用多个工具。\n",
    "\n",
    "<div><img src=\"imgs/basic-react.png\" width=\"600\"/></div>\n",
    "\n",
    "我们将选择实现 [**ReAct (Reason+Act)** 循环](https://arxiv.org/abs/2210.03629)，它做了一个简单的假设：\n",
    "\n",
    "**不断调用工具并观察工具调用的结果，直到得到最终答案。**\n",
    "\n",
    "有时这可以很明确，比如能实际给出“最终答案”和“询问用户”的工具；而其它时候，没调用工具时跳过用户也隐含地表明了这一点。我们将选择后者，因为入口现在支持会话和工具调用，所有这些都在一个请求中完成。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebcdd120",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import ToolMessage\n",
    "\n",
    "state = {\"messages\": []}\n",
    "agent_results = []\n",
    "\n",
    "## BEGIN EXERCISE\n",
    "\n",
    "while True:\n",
    "    try: \n",
    "        ## TODO: If a tool is not called, the answer-generating loop ends. \n",
    "        ##   When this happens, ASK THE USER FOR A NEW INPUT.\n",
    "        if not agent_results:\n",
    "            state[\"messages\"] += [(\"human\", input(\"\\n[Human]:\"))]\n",
    "\n",
    "        print(\"\\n[Agent]: \", end=\"\")\n",
    "        agent_response = None\n",
    "        for chunk in agent_chain.stream(state):\n",
    "            agent_response = chunk if not agent_response else agent_response + chunk\n",
    "            print(chunk.content, end=\"\")\n",
    "        print()\n",
    "\n",
    "        ## Get the agent message (pre-tool call), the function arguments, and the call invocation \n",
    "        agent_fncalls = [call.get(\"function\") for call in agent_response.additional_kwargs.get(\"tool_calls\", [])]\n",
    "        agent_results = [result.content for result in tool_caller.invoke(agent_response)]\n",
    "        if agent_fncalls: print(agent_fncalls)\n",
    "        if agent_results: print(agent_results)\n",
    "\n",
    "        ## TODO: If a tool is called, record it in the conversational history.\n",
    "        if not agent_results:\n",
    "            response = agent_response.content\n",
    "        else: \n",
    "            response = (\n",
    "                f\"{agent_response.content}\\n\"\n",
    "                f\"\\n<RESULT>{agent_results}</RESULT>\"\n",
    "            )\n",
    "        \n",
    "        state[\"messages\"] += [(\"ai\", response)]\n",
    "        \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"KeyboardInterrupt\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "575a1542",
   "metadata": {},
   "source": [
    "**潜在问题：** \n",
    "- (56766*30432+3043)/99 是多少？\n",
    "\n",
    "**寻找答案的过程：**\n",
    "- 第一步：1727502912\n",
    "- 第二步：1727505955\n",
    "- 第三步：17449555.101010103\n",
    "\n",
    "**挑战问题：**\n",
    "- 计算前 25 个斐波那契数。\n",
    "- 现在使用工具计算第 26 到 30 个数字。\n",
    "- 第 30 个数字比第 25 个数字大多少？"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936116d2",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<br>\n",
    "\n",
    "# <font color=\"#76b900\">**总结**</font>\n",
    "\n",
    "这部分，我们介绍了有状态的 LLM 系统以及支撑更复杂和可控（甚至试图自我控制）模型的逻辑！这仅仅是您可以使用 LLM 做的令人兴奋事情的开始，希望您能喜欢！\n",
    "\n",
    "从有限的任务特定编码器到强大的生成模型和自我引导模型的转变，希望您能意识到我们讨论的每个模型在整个架构中都有其位置！无论是关键的基础组件，具性价比的补充机制，还是进一步发展的灵感来源，或者是该领域进步的基石，尽量在今后的工作中保留这些工具，并使用那些最适合您环境的工具！\n",
    "\n",
    "**在下一个，也是最后的 notebook 中，我们将请您创建一个更专业的工作流，以便练习并扩展之前的技术！**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7259b79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Please Run When You're Done!\n",
    "import IPython\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1605574-67eb-4b93-97b2-d945cdc6ecdd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aec45f3-b45f-47dc-a9a8-aedafca0beea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
