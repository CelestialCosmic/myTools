{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "703486c2",
   "metadata": {},
   "source": [
    "<center><a href=\"https://www.nvidia.cn/training/\"><img src=\"https://dli-lms.s3.amazonaws.com/assets/general/DLI_Header_White.png\" width=\"400\" height=\"186\" /></a></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d13c262",
   "metadata": {},
   "source": [
    "# <font color=\"#76b900\"> **8:** 课程评估"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66bd592",
   "metadata": {},
   "source": [
    "**恭喜您（几乎）完成课程啦！** \n",
    "\n",
    "希望您喜欢这趟旅程，并获得了宝贵的技能，可以开始构建新颖且有趣的语言应用。现在，是时候把这些技能用于评估了！\n",
    "\n",
    "在之前的部分，我们探讨了语言模型的各个方面，包括数据工作流工具、视觉语言模型（VLM）和扩散模型。评估中，我们将把所有这些概念结合起来，构建一个有趣的应用，您在使用图像生成器时可能已经理所当然地假设它的存在了。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef47e164",
   "metadata": {},
   "source": [
    "### **设置**\n",
    "\n",
    "在开始之前，让我们导入必要的库并初始化语言模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a58d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "\n",
    "## USE THIS ONE TO START OUT WITH. NOTE IT'S INTENTED USE AS A VISUAL LANGUAGE MODEL FIRST\n",
    "# model_path=\"http://localhost:9000/v1\"\n",
    "## USE THIS ONE FOR GENERAL USE AS A SMALL-BUT-PURPOSE CHAT MODEL BEING RAN LOCALLY VIA NIM\n",
    "model_path=\"http://nim:8000/v1\"\n",
    "# ## USE THIS ONE FOR ACCESS TO CATALOG OF RUNNING NIM MODELS IN `build.nvidia.com`\n",
    "# model_path=\"http://llm_client:9000/v1\"\n",
    "\n",
    "model_name = requests.get(f\"{model_path}/models\").json().get(\"data\", [{}])[0].get(\"id\")\n",
    "%env NVIDIA_BASE_URL=$model_path\n",
    "%env NVIDIA_DEFAULT_MODE=open\n",
    "\n",
    "if \"llm_client\" in model_path:\n",
    "    model_name = \"meta/llama-3.1-70b-instruct\"\n",
    "\n",
    "llm = ChatNVIDIA(model=model_name, base_url=model_path, max_tokens=5000, temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "090034d9",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<br>\n",
    "\n",
    "## **8.1：** 评估\n",
    "\n",
    "在课程评估中，您将实现一个通常在图像生成 API 后面的常见功能；**合成提示词**。\n",
    "\n",
    "在创建以文本为条件的扩散模型时，开发者通常会创建一个合成的富含关键词的数据集用于训练，以便让模型学到强大的定制先验。\n",
    "- **在理想情况下，**一个图像生成器可以通过提示生成一幅任意的高质量图像，完美契合任何自然语言提示——只要提示本身足够有表现力。\n",
    "- **实际中，**图像生成器通常会被松散的指令提示——有时也会进行训练——模型根据其训练数据对细节做出广泛的假设。这通常表现为“图像检索”，即仅对训练数据进行微小修改后生成的结果。\n",
    "\n",
    "大多数提供商更希望让人们自由去提示模型，因此许多人选择使用从文本映射到文本的接口，将“常规人类提示”映射到“扩散输入提示”空间。\n",
    "\n",
    "### **练习：** 受图像启发的生成\n",
    "\n",
    "在课程练习中，您将实现一个潜在的模式，将合成提示词与视觉结合，以“创建受另一幅图像启发的图像”。该过程将分解为以下任务，并需要在最后合并以通过评估。\n",
    "\n",
    "<div><img src=\"imgs/rad-assessment.png\" width=\"800\"/></div>\n",
    "\n",
    "**注意：**任务 1 到 3 仅是实现任务 4 最终方案的构建模块。只有任务 4 的结果会被评分。如果您有信心，可以随意跳过任务 1 到 3。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7431e0f2",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<br>\n",
    "\n",
    "### **[任务 1]** 接收图像\n",
    "\n",
    "首先，我们需要能够接收和推理图像。为此，请实现下面的 `ask_about_image` 方法：\n",
    "\n",
    "**简化假设：**\n",
    "- 让您的 LLM 预测一个问题和一个图像文件应该不会太难，但也可以直接硬编码。\n",
    "- 根据您图像池的更新频率，最好结合一些批处理、缓存、分组和预处理的方式。也可以不进行这些优化。\n",
    "- LangChain 连接器如 `ChatNVIDIA` 和 `ChatOpenAI` 确实提供了简化的接口，但也可以重复使用之前的请求代码。请选择您认为最简单的方式。\n",
    "\n",
    "**提示：**\n",
    "- 回忆一下 notebook 6 中的 VLM。也许可以用那个模型？它还在吗？\n",
    "- 额外说明一下，一些模型如 GPT-4o 作为聊天和图像推理模型都非常强大。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3153c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import base64\n",
    "\n",
    "def ask_about_image(image_path, question=\"Describe the image\"):\n",
    "    ####################################################################\n",
    "    ## < EXERCISE SCOPE\n",
    "\n",
    "    ## TODO: Implement the method by connecting to a vision-language model\n",
    "\n",
    "    ## EXERCISE SCOPE >\n",
    "    ####################################################################\n",
    "\n",
    "description = ask_about_image(\"imgs/agent-overview.png\", \"Describe the image\")\n",
    "print(description)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e54c27",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<br>\n",
    "\n",
    "### **[任务 2]** 图像创建\n",
    "\n",
    "现在我们有了图像描述，试着根据这个响应生成一幅图像:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab2bfb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import DiffusionPipeline\n",
    "import torch\n",
    "\n",
    "## TODO: Consider initializing your diffusion pipeline outside of generate_images\n",
    "\n",
    "## TODO: Implement this method\n",
    "def generate_images(prompt):\n",
    "    ####################################################################\n",
    "    ## < EXERCISE SCOPE\n",
    "    return []\n",
    "    ## EXERCISE SCOPE >\n",
    "    ####################################################################\n",
    "\n",
    "images = generate_images(description)\n",
    "for img in images:\n",
    "    img.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4029f26",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<br>\n",
    "\n",
    "### **[任务 3]** 提示词合成\n",
    "\n",
    "经过初步尝试后，您应该注意到这种描述太复杂了。也许如果我们通过一个支持 LLM 的接口将这两个不同的组件连接起来，能得到更好的结果？\n",
    "\n",
    "抽象来讲，这个接口帮助将 VLM 输出域映射到扩散输入域，但实际上可以在任何两个规范之间完成。**简单来说，我们是想通过一个中间步骤，将 VLM 描述映射到扩散提示词。**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f51638-84aa-4542-a6fd-f86d1a7d6c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 解题思路\n",
    "# from langchain_core.output_parsers import StrOutputParser\n",
    "# from langchain_core.prompts import ChatPromptTemplate\n",
    "# from langchain_core.runnables import RunnableBranch\n",
    "# from random import random\n",
    "\n",
    "# sys_msg = (\n",
    "#     \"Please help the user. After every response, output '[stop]` if the conversation should end and [pass] otherwise.\"\n",
    "# )\n",
    "\n",
    "# prompt = ChatPromptTemplate.from_messages([(\"system\", sys_msg), (\"placeholder\", \"{messages}\")])\n",
    "# chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "# state = {\"messages\": []}\n",
    "# agent_msg = \"\"\n",
    "\n",
    "# while True:\n",
    "#     try: \n",
    "#         if \"[stop]\" in agent_msg: break\n",
    "#         else: pass\n",
    "        \n",
    "#         ## TODO: Update the messages appropriately\n",
    "#         human_msg = input(\"\\n[Human]:\")\n",
    "#         state[\"messages\"] += [(\"human\", human_msg)]\n",
    "\n",
    "#         ## Initiate an agent buffer to accumulate agent response\n",
    "#         agent_msg = \"\"\n",
    "#         print(\"\\n[Agent]: \", end=\"\")\n",
    "#         ## TODO: Stream the LLM's response directly to output and accumulate it\n",
    "#         for token in chain.stream(state):\n",
    "#             agent_msg += token\n",
    "#             print(token, end=\"\")\n",
    "\n",
    "#         ## TODO: Update the messages list appropriately\n",
    "#         state[\"messages\"] += [(\"ai\", agent_msg)]\n",
    "#     except KeyboardInterrupt:\n",
    "#         print(\"KeyboardInterrupt\")\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b47cb31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "####################################################################\n",
    "## < EXERCISE SCOPE\n",
    "\n",
    "## TODO: Create a pipeline for synthetic prompts, optputting a string.\n",
    "\n",
    "## EXERCISE SCOPE >\n",
    "####################################################################\n",
    "\n",
    "new_diff_prompt = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f8427e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = generate_images(new_diff_prompt)\n",
    "for img in images:\n",
    "    img.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa66bd2",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<br>\n",
    "\n",
    "### **[任务 4]** 工作流和迭代\n",
    "\n",
    "**为了完成评估，将这些任务整合到一个工作流中，使以下过程变得一键化：**\n",
    "- **从计算环境中获取一幅图像。**\n",
    "- **计算图像的摘要。**\n",
    "- **使用 LLM 为图像生成工作流创建四个不同的合成提示词。**\n",
    "- **生成四幅您满意的独特图像。**\n",
    "\n",
    "**注意：**\n",
    "- 可以选择将其实现为标准函数或链。\n",
    "- 返回一个 PIL 图像数组。您可以选择默认显示它们。\n",
    "- 为了加快处理速度，建议尝试并行处理和批处理。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "33120dcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The image presents a diagram illustrating the relationship between an agent and its environment. The agent is represented by a red rectangle, and it is connected to various components of its environment through solid and dashed lines. The environment is divided into three main sections: Tools, Memory, and Action. The Tools section includes items such as a Calendar, Calculator, and CodeInterpreter, among others. The Memory section is further divided into Short-term memory and Long-term memory. The Action section is connected to the Tools and Memory sections through dashed lines, indicating a less direct relationship. Additionally, there are sub-components within the Memory and Action sections, such as Reflection, Self-critics, Chain of thoughts, and Subgoal decomposition, which are connected to the main sections with solid lines. The overall layout suggests a flow of interaction between the agent and its environment, with the agent using tools and memory to perform actions and reflect on its processes.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "dictionary update sequence element #0 has length 128; 2 is required",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 126\u001b[0m\n\u001b[1;32m    123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m generated_images, diffusion_prompts, original_description\n\u001b[1;32m    125\u001b[0m results \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m--> 126\u001b[0m results \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m [\u001b[43mgenerate_images_from_image\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mimgs/agent-overview.png\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m]\n\u001b[1;32m    127\u001b[0m results \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m [generate_images_from_image(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimgs/multimodal.png\u001b[39m\u001b[38;5;124m\"\u001b[39m)]\n\u001b[1;32m    128\u001b[0m results \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m [generate_images_from_image(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimg-files/tree-frog.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m)]\n",
      "Cell \u001b[0;32mIn[7], line 70\u001b[0m, in \u001b[0;36mgenerate_images_from_image\u001b[0;34m(image_url, num_images)\u001b[0m\n\u001b[1;32m     55\u001b[0m diffusion_prompts \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     58\u001b[0m chat_prompt \u001b[38;5;241m=\u001b[39m ChatPromptTemplate\u001b[38;5;241m.\u001b[39mfrom_messages([\n\u001b[1;32m     59\u001b[0m     (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\n\u001b[1;32m     60\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are a helpful DLI Chatbot who can request and reason about notebooks.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     66\u001b[0m     (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mplaceholder\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{messages}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     67\u001b[0m ])\n\u001b[1;32m     69\u001b[0m pipeline \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m---> 70\u001b[0m     RunnablePassthrough\u001b[38;5;241m.\u001b[39massign(full_context \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdescription\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;241m|\u001b[39m chat_prompt \n\u001b[1;32m     72\u001b[0m     \u001b[38;5;241m|\u001b[39m llm \n\u001b[1;32m     73\u001b[0m     \u001b[38;5;241m|\u001b[39m StrOutputParser()\n\u001b[1;32m     74\u001b[0m )\n\u001b[1;32m     76\u001b[0m chat_state \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;66;03m# \"filenames\": [\"07_intro_agentics.ipynb\"],\u001b[39;00m\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;66;03m# \"messages\": [(\"human\", \"Can you give me a summary of the notebook?\")],\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     85\u001b[0m     )],\n\u001b[1;32m     86\u001b[0m }\n\u001b[1;32m     88\u001b[0m short_summary \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mValueError\u001b[0m: dictionary update sequence element #0 has length 128; 2 is required"
     ]
    }
   ],
   "source": [
    "## TODO: Execute on assessment objective\n",
    "import requests\n",
    "import base64\n",
    "import os \n",
    "from chatbot.conv_tool_caller import ConversationalToolCaller\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.pydantic_v1 import Field\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from chatbot.jupyter_tools import FileLister\n",
    "def generate_images_from_image(image_url: str, num_images = 4):\n",
    "\n",
    "    ####################################################################\n",
    "    ## < EXERCISE SCOPE\n",
    "\n",
    "\n",
    "    invoke_url = \"http://localhost:9000/v1/chat/completions\"\n",
    "    stream = False\n",
    "    with open(image_url, \"rb\") as f:\n",
    "      image_b64 = base64.b64encode(f.read()).decode()\n",
    "    headers = {\n",
    "        \"Authorization\": \"Bearer $API_KEY_REQUIRED_IF_EXECUTING_OUTSIDE_NGC\",\n",
    "        \"Accept\": \"text/event-stream\" if stream else \"application/json\"\n",
    "    }\n",
    "    \n",
    "    payload = {\n",
    "        \"model\": 'microsoft/phi-3.5-vision-instruct',\n",
    "        \"messages\": [\n",
    "            {'role': 'system', 'content': 'Describe the image'},\n",
    "            {'role': 'user', 'content': [\n",
    "                {'type': 'image_url', 'image_url': {'url': f'data:image/jpeg;base64,{image_b64}', 'detail': 'low'}}\n",
    "            ]},\n",
    "        ],\n",
    "        \"max_tokens\": 512,\n",
    "        \"temperature\": 0.20,\n",
    "        \"top_p\": 0.70,\n",
    "        \"stream\": stream\n",
    "    }\n",
    "    description = requests.post(invoke_url, headers=headers, json=payload)\n",
    "    desc = description.json()[\"choices\"][0][\"message\"][\"content\"]\n",
    "    print(desc)\n",
    "    ####################################################################\n",
    "    ## < EXERCISE SCOPE\n",
    "\n",
    "    ## TODO: Implement the method by connecting to a vision-language model\n",
    "\n",
    "    ## EXERCISE SCOPE >\n",
    "    ####################################################################\n",
    "    ## TODO: Generate the description of the image provided in image_url\n",
    "    original_description = description\n",
    "\n",
    "    ## TODO: Generate four disjoint prompts, hopefully different, to feed into SDXL\n",
    "    diffusion_prompts = []\n",
    "\n",
    "\n",
    "    chat_prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", (\n",
    "            \"You are a helpful DLI Chatbot who can request and reason about notebooks.\"\n",
    "            \" Be as concise as necessary, but follow directions as best as you can.\"\n",
    "            \" Please help the user out by answering any of their questions and following their instructions.\"\n",
    "        )),\n",
    "        (\"human\", \"Here is the notebook I want you to work with: {full_context}. Remembering this, start the conversation over.\"),\n",
    "        (\"ai\", \"Awesome! I will work with this as context and will restart the conversation.\"),\n",
    "        (\"placeholder\", \"{messages}\")\n",
    "    ])\n",
    "    \n",
    "    pipeline = (\n",
    "        RunnablePassthrough.assign(full_context = dict(description))\n",
    "        | chat_prompt \n",
    "        | llm \n",
    "        | StrOutputParser()\n",
    "    )\n",
    "    \n",
    "    chat_state = {\n",
    "        # \"filenames\": [\"07_intro_agentics.ipynb\"],\n",
    "        # \"messages\": [(\"human\", \"Can you give me a summary of the notebook?\")],\n",
    "        ## Reason about the entire course at once. This will be much slower and does not scale to larger document pools. \n",
    "        \"filenames\": filenames, \n",
    "        \"messages\": [(\"human\", \n",
    "            \"Can you give me a summary of the course, making sure to mention every notebook?\"\n",
    "            \" Do a paragraph per notebook, and finish by explaining big-picture ideas to help an\"\n",
    "            \" instructor explain the material and understand which parts of the course to refer to when addressing questions.\"\n",
    "        )],\n",
    "    }\n",
    "    \n",
    "    short_summary = \"\"\n",
    "    for chunk in pipeline.stream(chat_state):\n",
    "        print(chunk, end=\"\")\n",
    "        short_summary += chunk\n",
    "    \n",
    "        ## TODO: Generate the resulting images\n",
    "        generated_images = []\n",
    "    \n",
    "    \n",
    "    from diffusers import DiffusionPipeline\n",
    "    import torch\n",
    "    desc = description.json()[\"choices\"][0][\"message\"][\"content\"]\n",
    "    ## TODO: Consider initializing your diffusion pipeline outside of generate_images\n",
    "    \n",
    "    ## TODO: Implement this method\n",
    "    \n",
    "    ####################################################################\n",
    "    ## < EXERCISE SCOPE\n",
    "    pipe = DiffusionPipeline.from_pretrained(\n",
    "    \"stabilityai/stable-diffusion-xl-base-1.0\",\n",
    "    torch_dtype=torch.float16,\n",
    "    use_safetensors=True,\n",
    "    variant=\"fp16\",\n",
    "    ).to(\"cuda\")\n",
    "    images = pipe(prompt=desc).images\n",
    "    ## EXERCISE SCOPE >\n",
    "    ####################################################################\n",
    "    \n",
    "    images = generate_images(desc)\n",
    "\n",
    "    for img in images:\n",
    "        img.show()\n",
    "    ## EXERCISE SCOPE >\n",
    "    ####################################################################\n",
    "        \n",
    "    return generated_images, diffusion_prompts, original_description\n",
    "\n",
    "results = []\n",
    "results += [generate_images_from_image(\"imgs/agent-overview.png\")]\n",
    "results += [generate_images_from_image(\"imgs/multimodal.png\")]\n",
    "results += [generate_images_from_image(\"img-files/tree-frog.jpg\")]\n",
    "results += [generate_images_from_image(\"img-files/paint-cat.jpg\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ef55a5",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<br>\n",
    "\n",
    "## **8.2：** 运行评估"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d64db6d",
   "metadata": {},
   "source": [
    "要评估您的提交，请运行以下单元以保存您的结果，然后运行后面的单元查询评估运行器。\n",
    "\n",
    "**遵循指示，确保所有步骤通过。**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6547fe8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import requests\n",
    "from PIL import Image\n",
    "import re\n",
    "\n",
    "def save_images_and_metadata(results, save_dir=\"generated_images\"):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    # Collect all image paths and metadata\n",
    "    all_metadata = []\n",
    "    taken_idxs = [re.findall(r'\\d+', filename) for filename in os.listdir(save_dir)]\n",
    "    taken_idxs = [int(idx_list[0]) for idx_list in taken_idxs if idx_list]\n",
    "    start_idx = max(taken_idxs, default=0) + 1  # Find the next available index\n",
    "\n",
    "    for result in results:\n",
    "        images, prompts, original_description = result\n",
    "        img_paths = []\n",
    "\n",
    "        # Save each image and store its path\n",
    "        for idx, img in enumerate(images):\n",
    "            img_path = os.path.join(save_dir, f\"image_{start_idx + idx}.png\")\n",
    "            img.save(img_path, \"PNG\")\n",
    "            img_paths.append(img_path)\n",
    "        \n",
    "        # Append metadata for the current batch\n",
    "        all_metadata.append({\n",
    "            \"original_description\": original_description,\n",
    "            \"prompts\": prompts,\n",
    "            \"image_paths\": img_paths\n",
    "        })\n",
    "        start_idx += len(images)\n",
    "    \n",
    "    # Save all metadata in a single JSON file\n",
    "    metadata_path = os.path.join(save_dir, \"all_metadata.json\")\n",
    "    with open(metadata_path, 'w') as f:\n",
    "        json.dump(all_metadata, f, indent=4)\n",
    "    return all_metadata\n",
    "\n",
    "## Generate your submission\n",
    "submission = save_images_and_metadata(results)\n",
    "\n",
    "## Send the submission over to the assessment runner\n",
    "response = requests.post(\n",
    "    \"http://docker_router:8070/run_assessment\", \n",
    "    json={\"submission\": submission},\n",
    ")\n",
    "\n",
    "response.raise_for_status()\n",
    "\n",
    "try: \n",
    "    print(response.json().get(\"result\"))\n",
    "    if response.json().get(\"messages\"):\n",
    "        print(\"MESSAGES:\", \"\\n- \".join([\"\"] + response.json().get(\"messages\")))\n",
    "    if response.json().get(\"exceptions\"):\n",
    "        print(\"EXCEPTIONS:\", \"\\n- \".join([\"\"] + [str(v) for v in response.json().get(\"exceptions\")]))\n",
    "except:\n",
    "    print(\"Failed To Process Assessment Response\")\n",
    "    print(response.__dict__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22141047",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "如果您通过了评估，请返回课程页面（如下所示），点击**\"ASSESS TASK\"**按钮，这将为您生成课程证书。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca05258f",
   "metadata": {},
   "source": [
    "<img src=\"./imgs/assess_task.png\" style=\"width: 800px;\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7589187a",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<br>\n",
    "\n",
    "## **8.3：** 总结\n",
    "\n",
    "### <font color=\"#76b900\">**恭喜您完成课程!!**</font>\n",
    "\n",
    "在结束课程之前，我们强烈建议您下载课程材料以便后续参考，然后看看课程的**\"下一步\"**和**反馈**部分。**感谢您抽出时间参加课程，期待在系列课程的下一个阶段再见到您！**\n",
    "\n",
    "<br>\n",
    "<hr>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
