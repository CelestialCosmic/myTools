{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c10ae1a8",
   "metadata": {},
   "source": [
    "<center><a href=\"https://www.nvidia.cn/training/\"><img src=\"https://dli-lms.s3.amazonaws.com/assets/general/DLI_Header_White.png\" width=\"400\" height=\"186\" /></a></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79039684",
   "metadata": {},
   "source": [
    "# <font color=\"#76b900\"> **7.5:** LangGraph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2cbc955",
   "metadata": {},
   "source": [
    "**恭喜您（几乎）完成课程！** \n",
    "\n",
    "希望您在这个过程中获得了创建高级语言模型应用的宝贵技能。 \n",
    "- **在 notebook 8 中，** 您将能够将这些技能付诸实践，构建一个跨越多个领域的集成系统。 \n",
    "- **在这个 notebook 中，** 我们将简要介绍 LangGraph，这是一个流行的多智能体编排框架，它做出了一些有用的设计决策，对于希望深入这个领域的人来说是一个绝佳的起点！"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c70131",
   "metadata": {},
   "source": [
    "### **设置**\n",
    "\n",
    "在开始之前，让我们导入必要的库并初始化我们的语言模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70565056",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "## USE THIS ONE TO START OUT WITH. NOTE IT'S INTENTED USE AS A VISUAL LANGUAGE MODEL FIRST\n",
    "# model_path=\"http://localhost:9000/v1\"\n",
    "## USE THIS ONE FOR GENERAL USE AS A SMALL-BUT-PURPOSE CHAT MODEL BEING RAN LOCALLY VIA NIM\n",
    "model_path=\"http://nim:8000/v1\"\n",
    "# ## USE THIS ONE FOR ACCESS TO CATALOG OF RUNNING NIM MODELS IN `build.nvidia.com`\n",
    "# model_path=\"http://llm_client:9000/v1\"\n",
    "\n",
    "model_name = requests.get(f\"{model_path}/models\").json().get(\"data\", [{}])[0].get(\"id\")\n",
    "%env NVIDIA_BASE_URL=$model_path\n",
    "%env NVIDIA_DEFAULT_MODE=open\n",
    "\n",
    "if \"llm_client\" in model_path:\n",
    "    model_name = \"meta/llama-3.1-70b-instruct\"\n",
    "\n",
    "llm = ChatNVIDIA(model=model_name, base_url=model_path, max_tokens=5000, temperature=0, streaming=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3372446",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "最后，让我们加载 notebook 名称以及之前计算的 notebook 摘要字典。我们将在整个 notebook 中默认使用这些摘要，但您可以随意尝试。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b22410",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "with open('notebook_chunks.json', 'r') as fp:\n",
    "    nbsummary = json.load(fp)\n",
    "\n",
    "filenames = nbsummary.get(\"filenames\")\n",
    "outlines = \"\\n\\n\".join([v.get(\"outline\") for k,v in nbsummary.items() if isinstance(v, dict)])\n",
    "# outlines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94454ed9",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<br>\n",
    "\n",
    "## **8.1：智能 notebook 检索**\n",
    "\n",
    "在评估的第一部分中——旨在展示智能工作流的新抽象——我们将创建一个能够从 notebook 中检索信息并与用户进行有意义交互的智能体。\n",
    "\n",
    "**具体来说，我们的智能体将：**\n",
    "- 与用户交互以理解他们的查询。\n",
    "- 从一组 Jupyter notebook 中访问和检索信息。\n",
    "- 根据检索到的信息提供简洁且有帮助的响应。\n",
    "\n",
    "为了有一个不错的起点，让我们重新创建上一个 notebook 中的对话智能体，并在这里保留我们的提示词以方便自定义："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f14a738",
   "metadata": {},
   "outputs": [],
   "source": [
    "from chatbot.conv_tool_caller import ConversationalToolCaller\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.pydantic_v1 import Field\n",
    "\n",
    "\n",
    "tool_instruction: str = (\n",
    "    \"In addition to your directive, you have access to the tools listed in the toolbank.\"\n",
    "    \" Use tools only within the \\n<function></function> tags.\"\n",
    "    \" Select tools to handle uncertain, imprecise, or complex computations that an LLM would find it hard to answer.\"\n",
    "    \" You can only call one tool at a time, and the tool cannot accept complex multi-step inputs.\"\n",
    "    \"\\n\\n<toolbank>{toolbank}</toolbank>\\n\"\n",
    "    \"Examples (WITH HYPOTHETICAL TOOLS):\"\n",
    "    \"\\nSure, let me call the tool in question.\\n<function=\\\"foo\\\">[\\\"input\\\": \\\"hello world\\\"]</function>\"\n",
    "    \"\\nSure, first, I need to calculate the expression of 5 + 10\\n<function=\\\"calculator\\\">[\\\"expression\\\": \\\"5 + 10\\\"]</function>\"\n",
    "    \"\\nSure! Let me look up the weather in Tokyo\\n<function=\\\"weather\\\">[\\\"location\\\"=\\\"Tokyo\\\"])</function>\"\n",
    ")\n",
    "\n",
    "tool_prompt: str = (\n",
    "    \"You are an expert at selecting tools to answer questions. Consider the context of the problem,\"\n",
    "    \" what has already been solved, and what the immediate next step to solve the problem should be.\"\n",
    "    \" Do not predict any arguments which are not present in the context; if there's any ambiguity, use no_tool.\"\n",
    "    \"\\n\\n<toolbank>{toolbank}</toolbank>\\n\"\n",
    "    \"\\n\\nSchema Instructions: The output should be formatted as a JSON instance that conforms to the JSON schema.\"\n",
    "    \"\\n\\nExamples (WITH HYPOTHETICAL TOOLS):\"\n",
    "    \"\\n<function=\\\"search\\\">[\\\"query\\\": \\\"current events in Japan\\\"]</function>\"\n",
    "    \"\\n<function=\\\"translation\\\">[\\\"text\\\": \\\"Hello, how are you?\\\", \\\"language\\\": \\\"French\\\"]</function>\"\n",
    "    \"\\n<function=\\\"calculator\\\">[\\\"expression\\\": \\\"5 + 10\\\"]</function>\"\n",
    ")\n",
    "\n",
    "conv_llm = ConversationalToolCaller(\n",
    "    tool_instruction=tool_instruction, \n",
    "    tool_prompt=tool_prompt, \n",
    "    llm=llm\n",
    ").get_tooled_chain()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d25867",
   "metadata": {},
   "source": [
    "在您进行修改时，请随意调整这些提示词，因为您可以进行各种调整以提高智能体的性能。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819aa338",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### **7.5.1：介绍 LangGraph**\n",
    "\n",
    "**[LangGraph 框架](https://github.com/langchain-ai/langgraph)**是一个新增加的工具，它允许我们使用状态图来管理对话流。通过 LangGraph，我们能以结构化的方式定义智能体的状态、转换和动作，省去了完全自定义事件循环的需要。这个框架增强了可扩展性和可维护性，特别是在处理多智能体系统或复杂工作流时。\n",
    "\n",
    "#### LangGraph 如何增强我们的工作流：\n",
    "- **状态管理：**LangGraph 允许清晰地区分对话中的不同状态，使得跟踪和管理智能体的进展和决策变得更容易。\n",
    "- **条件转换：**使用 LangGraph，我们可以定义条件边，根据特定的触发器或条件来决定对话的流向。\n",
    "- **模块化：**该框架通过允许不同节点（函数）处理特定任务来促进模块化，从而便于更新和扩展。\n",
    "\n",
    "#### 为什么 LangGraph 比自定义方案更好？\n",
    "- **为多智能体系统设计：**与我们可以调整为可行的多状态系统的 while 循环不同，LangGraph 采用状态图的方法来建模智能体的遍历过程。因此，它结合了自然扩展到非顺序甚至动态例程的设计模式。\n",
    "- **简化的集成：**作为一个相对流行的框架，LangGraph 积累了大量免费和付费的集成，极大地改善了开发和部署体验。开发团队发布了像 LangServe、LangSmith 和 LangGraph-Studio 的集成，整个社区也贡献了多种示例应用，展示了特定领域的应用和模块化的即插即用组件。如果您想要一个相对新颖的智能体范例，很多人可能会用 LangGraph 实现。\n",
    "\n",
    "#### LangGraph 在什么情况下不如自定义方案？\n",
    "- **可能过度设计：**为了考虑各种多智能体特定的功能集和边缘案例，LangGraph 实施了一些强假设，这大大增加了它的学习曲线。如果您能在基本的 LangChain 中实现解决方案，那么运行时范式就足够简化您的工作流，避免 LangGraph 引入的多层复杂性。而如果您知道想要扩展应用并能受益于其深思熟虑的功能/示例，那么也许值得深入了解并熟悉它。\n",
    "- **受限的抽象：**尽管 LangGraph 非常出色，但在 LangGraph 之外仍然有更深层次的优化和更强的模块化空间。那些希望构建高度专业化微服务的人可能会对自定义多线程/多进程方案、先进的图算法和高级资源管理策略感兴趣，而这些 LangGraph 可能无法提供。对此感兴趣的人，可以参考 [**Knowledge-Graph-RAG**](https://github.com/NVIDIA/GenerativeAIExamples/tree/main/community/knowledge_graph_rag)。\n",
    "\n",
    "----\n",
    "\n",
    "在接下来的 notebook 中，我们将使用 LangGraph 来管理智能体与其工具集之间的流动，以重现之前的手动循环。虽然我们的应用可能没有复杂到必须要用它，但练习 LangGraph 有助于加速您对更大多智能体生态系统的掌握。\n",
    "\n",
    "接下来，让我们定义一个典型的图，将人类输入节点与智能体响应节点连接起来："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154d0b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import END, StateGraph, START\n",
    "from langgraph.prebuilt import tools_condition\n",
    "from langgraph.graph.message import AnyMessage, add_messages\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "from typing import Annotated\n",
    "from typing_extensions import TypedDict\n",
    "import operator\n",
    "import uuid\n",
    "import datetime\n",
    "from IPython.display import Image, display\n",
    "\n",
    "##################################################################\n",
    "\n",
    "class State(TypedDict):\n",
    "    ## Dictates what kind of buffer the agent nodes can write to to pass information\n",
    "    ## This one says \"nodes can write to messages buffer, writing is equivalent to adding a message\"\n",
    "    ## NOTE: To override a message, you can add a message with the target message's ID. \n",
    "    ## NOTE: To delete a message, you can add a RemoveMessage with the target's message ID. \n",
    "    messages: Annotated[list[AnyMessage], add_messages]\n",
    "    directives: Annotated[list[AnyMessage], add_messages]\n",
    "\n",
    "def create_graph(nodes, edges, conditional_edges=[], state=State, thread_id=\"42\", plot=True):\n",
    "    graph = StateGraph(state)\n",
    "    [graph.add_node(*node) for node in nodes]\n",
    "    [graph.add_edge(*edge) for edge in edges]\n",
    "    [graph.add_conditional_edges(*cedge) for cedge in conditional_edges]\n",
    "    \n",
    "    ## The checkpointer lets the graph persist its state\n",
    "    ## Thread used to select buffer / memory compartment / etc to operate on \n",
    "    config = {\"configurable\": {\"thread_id\": thread_id}}\n",
    "    memory = MemorySaver()\n",
    "    app = graph.compile(checkpointer=memory)\n",
    "\n",
    "    if plot: display(Image(app.get_graph(xray=True).draw_mermaid_png()))\n",
    "    return app, memory, config\n",
    "\n",
    "#################################################################\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", (\n",
    "        \"You are a helpful DLI (Deep Learning Institute) Chatbot who can request and reason about notebooks.\"\n",
    "        \" Be as concise as necessary, but follow directions as best as you can.\"\n",
    "        \" Please help the user out by answering any of their questions and following their instructions.\"\n",
    "    )),\n",
    "    (\"human\", f\"Here is the info I want you to work with for all future correspondences: {outlines}\"),\n",
    "    (\"ai\", \"Awesome! I will proceed from scratch with this understanding.\"),\n",
    "    (\"placeholder\", \"{messages}\")\n",
    "])\n",
    "\n",
    "\n",
    "def human_fn(state):\n",
    "    ## Simple function to get user input and publish it to LangGraph message buffer\n",
    "    return {\"messages\": (\"human\", input(\"[Human]\"))} ## Adds to the message buffer associated with current thread\n",
    "\n",
    "\n",
    "async def assistant_fn(state, config: RunnableConfig, **kwargs):\n",
    "    ## Agent response function, which prompts the LLM with the message buffer and writes the results\n",
    "    chain = chat_prompt | llm | StrOutputParser()\n",
    "    response = await chain.ainvoke(state, config)    ## Config has callbacks which intercept stream generation\n",
    "    return {\"messages\": [(\"ai\", response)]}          ## Adds to the message buffer associated with current thread\n",
    "\n",
    "\n",
    "app, memory, config = create_graph(\n",
    "    ## These are the functions we want to include in our graph.\n",
    "    nodes = [\n",
    "        (\"assistant\", assistant_fn), \n",
    "        (\"human\", human_fn)\n",
    "    ],\n",
    "    ## These are the edges that connect our nodes to define agentic flow.\n",
    "    edges = [\n",
    "        (START, \"human\"),\n",
    "        (\"human\", \"assistant\"),\n",
    "        (\"assistant\", END),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55fbd30e",
   "metadata": {},
   "source": [
    "要从这个编译的图中流式传输，您可以*大致*将其用作一个运行时，并进行一些额外的配置和选项："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba89737",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Invoking from compiled LG app\n",
    "output = await app.ainvoke({\"messages\": []}, config=config)\n",
    "print(output.get(\"messages\")[-1].content)\n",
    "\n",
    "## Streaming from compiled LG app\n",
    "async for msg, meta in app.astream({\"messages\": []}, stream_mode=\"messages\", config=config):\n",
    "    print(msg.content, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f786f1bf",
   "metadata": {},
   "source": [
    "**在您的探索中，请注意以下功能:**\n",
    "- 编译的图保持了对话历史！这是因为我们有一个**检查点（checkpointer）**（在图编译时指定），它在后台跟踪线程为“42”的对话（由我们的配置决定的）。请注意，检查点对于涉及回溯、归档、人工参与等的集成也非常有用。\n",
    "- 您会注意到元数据中有一些不错的缓冲信息，这对于输出处理、过滤、归档等可能很有帮助。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb0599a6",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<br>\n",
    "\n",
    "### **7.5.2：重现我们的 ReAct 循环**\n",
    "\n",
    "在之前的 notebook 中，我们使用状态字典的自定义缓冲协议、while 循环和在响应未调用工具时触发的中断条件实现了基本的 ReAct 循环。在 LangGraph 中，这是一种常见的范式，通常用以下图形表示：\n",
    "\n",
    "> <div><img src=\"imgs/lg_react.png\" width=\"600\"/></div>\n",
    ">\n",
    "> **来源: [带结构化输出的 ReAct 智能体 | LangGraph 使用指南](https://langchain-ai.github.io/langgraph/how-tos/react-agent-structured-output/)**\n",
    "\n",
    "浏览这些资源时，您会发现多种实现方式，它们协同作用于节点逻辑、边缘逻辑和后处理逻辑，以构建一个连贯的流式系统。\n",
    "\n",
    "为了补充这些范式，我们可以将之前的想法在这里重新应用，并做一些关键修改：\n",
    "- 为了与我们之前的 while 循环方法保持一致，提供了一个集成的 `react` 示例。\n",
    "- 由于 while 循环方法通过合并可能合理的中间状态节点，而失去灵活性，所以提供了一个更模块化的`智能体 + 工具`选项，推荐您使用。\n",
    "- 为了避免重新指定流式处理程序，定义了一个轻量级的流式函数。\n",
    "\n",
    "在这个单元之后，您将不需要再次实现这些组件；只需将它们引入并参数化以适应您的需求。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f59999e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import tool\n",
    "from typing import Literal\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langchain_core.messages import ToolMessage\n",
    "from functools import partial\n",
    "\n",
    "################################################################################################\n",
    "# ## Combined agent + tools. Less flexible\n",
    "# async def react_fn(state, config: RunnableConfig, llm = conv_llm, tool_node = None, **kwargs):\n",
    "#     chain = chat_prompt | llm.bind(config=config)\n",
    "#     out_msgs = []\n",
    "#     while True:\n",
    "#         new_state = {**state, \"messages\": state.get(\"messages\") + out_msgs}\n",
    "#         response = await chain.ainvoke(new_state)\n",
    "#         out_msgs += [response]\n",
    "#         if response.tool_calls:\n",
    "#             out_msgs += [\n",
    "#                 f\"\\n<RESULT>\\n{result}\\n</RESULT>\" \n",
    "#                 for result in tool_node.invoke({\"messages\": [response]})[\"messages\"]\n",
    "#             ]\n",
    "#         else: \n",
    "#             break\n",
    "#     return {\"messages\": out_msgs}\n",
    "################################################################################################\n",
    "\n",
    "async def set_directive_fn(state, config: RunnableConfig):\n",
    "    return {\"directives\": [state.get(\"messages\")[-1]]}\n",
    "    \n",
    "\n",
    "async def agent_fn(\n",
    "    state, config: RunnableConfig, \n",
    "    llm = conv_llm, chat_prompt = chat_prompt, **kwargs\n",
    "):\n",
    "    chain = chat_prompt | llm\n",
    "    response = await chain.ainvoke(state, config=config)\n",
    "    ## This invocation makes a new message, so this return is an appending of a new message\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "    \n",
    "async def tools_fn(\n",
    "    state, config: RunnableConfig, \n",
    "    tool_node = (lambda x: x), **kwargs\n",
    "):\n",
    "    last_msg = state.get(\"messages\")[-1]\n",
    "    if last_msg.tool_calls:\n",
    "        results = tool_node.invoke({\"messages\": [last_msg]})[\"messages\"]\n",
    "        for result in results:\n",
    "            last_msg.content += f\"\\n<RESULT>{result.content}</RESULT>\"\n",
    "\n",
    "    directive = state.get(\"directives\")[-1].content\n",
    "    new_msgs = [last_msg, (\n",
    "        \"human\", f\"Great! Now continue responding to the original user directive: {directive}.\"\n",
    "            \" You've executed at least one tool, so continue your thought process. DO NOT redo any past processes.\"\n",
    "    )]\n",
    "    return {\"messages\": new_msgs}\n",
    "\n",
    "################################################################################################\n",
    "\n",
    "def loop_or_end(state: Literal[\"loop\", \"end\"], config: RunnableConfig):\n",
    "    ## Return the state to route to based on whether a tool is called\n",
    "    return \"loop\" if state.get(\"messages\")[-1].tool_calls else \"end\"\n",
    "\n",
    "app, memory, config = create_graph(\n",
    "    nodes = [\n",
    "        (\"enter\", set_directive_fn), \n",
    "        (\"agent\", agent_fn), \n",
    "        (\"tools\", tools_fn), \n",
    "        # (\"react\", react_fn), \n",
    "    ],\n",
    "    edges = [\n",
    "        (START, \"enter\"),\n",
    "        (\"enter\", \"agent\"),\n",
    "        (\"tools\", \"agent\"),\n",
    "        # (START, \"react\"), (\"react\", END),\n",
    "    ],\n",
    "    conditional_edges = [\n",
    "        (\"agent\", loop_or_end, {\"loop\": \"tools\", \"end\": END})\n",
    "    ]\n",
    ")\n",
    "\n",
    "################################################################################################\n",
    "\n",
    "async def stream_response(\n",
    "    new_message,\n",
    "    app, config,\n",
    "    print_stream=False,  ## If true, print messages from buffer. Otherwise, just prints tokens. \n",
    "    truncate=200,        ## Maximum length to give to each streamed value\n",
    "    show_meta=True,      ## Whether to show message metadata i.e. buffer, producing node, etc.\n",
    "    silences_nodes=[]    ## Nodes whos' results you don't want to see\n",
    "):\n",
    "    buffers = {}\n",
    "    new_messages = {\"messages\": [(\"human\", new_message)]}\n",
    "    async for msg, meta in app.astream(new_messages, stream_mode=\"messages\", config=config):\n",
    "        if meta.get(\"langgraph_node\") in silences_nodes: continue\n",
    "        if msg.id not in buffers:\n",
    "            delim = \"*\" * 84\n",
    "            print(f\"\\n\\n{delim}\\n** Found {msg.__class__.__name__} with id {msg.id}\\n{delim}\")\n",
    "            if show_meta: print(f\"{meta}\\n{delim}\")\n",
    "        buffers[msg.id] = msg if not buffers.get(msg.id) else (buffers.get(msg.id) + msg)\n",
    "        if print_stream: \n",
    "            print(repr(msg) if not truncate else str(repr(msg))[:truncate])\n",
    "        elif not isinstance(msg, ToolMessage):\n",
    "            print(msg.content, end=\"\")\n",
    "\n",
    "################################################################################################\n",
    "\n",
    "await stream_response(\n",
    "    input(\"[Human]\"), \n",
    "    app, config, \n",
    "    print_stream=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858ccb89",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<br>\n",
    "\n",
    "### **7.5.3：为我们的智能体装备工具**\n",
    "\n",
    "鉴于我们所有的构建模块，现在可以在不需要太多代码的情况下创建一个初始的工具化 LLM 智能体。为了举例说明一个入门智能体，我们将提供一个简单但强大的工具：`read_notebook`。这将允许智能体根据需要丰富其上下文，获取 notebook 的完整内容。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c29435",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from typing import Literal\n",
    "from chatbot.jupyter_tools import FileLister\n",
    "\n",
    "@tool\n",
    "def read_notebook(\n",
    "    filename: str, \n",
    ") -> str:\n",
    "    \"\"\"Displays a file to yourself and the end-user. These files are long, so only use it as a last resort.\"\"\"\n",
    "    return FileLister().to_string(files=[filename], workdir=\".\")\n",
    "\n",
    "## Advanced Note: The schema can be strategically modified to tell the server how to grammar enforce\n",
    "## In this case, specifying the finite options for the files. \n",
    "## To discover this, try type-hinting filename: Literal[\"file1\", \"file2\"] and printing schema\n",
    "read_notebook.args_schema.schema()[\"properties\"][\"filename\"][\"enum\"] = filenames\n",
    "\n",
    "################################################################################################\n",
    "\n",
    "toolset = [read_notebook]\n",
    "tooled_agent_fn = partial(agent_fn, llm = conv_llm.bind_tools(toolset), chat_prompt = chat_prompt)\n",
    "tooled_tools_fn = partial(tools_fn, tool_node = ToolNode(toolset))\n",
    "\n",
    "################################################################################################\n",
    "\n",
    "app, memory, config = create_graph(\n",
    "    nodes = [\n",
    "        (\"enter\", set_directive_fn), \n",
    "        (\"agent\", tooled_agent_fn), \n",
    "        (\"tools\", tooled_tools_fn), \n",
    "    ],\n",
    "    edges = [\n",
    "        (START, \"enter\"),\n",
    "        (\"enter\", \"agent\"),\n",
    "        (\"tools\", \"agent\"),\n",
    "    ],\n",
    "    conditional_edges = [\n",
    "        (\"agent\", loop_or_end, {\"loop\": \"tools\", \"end\": END})\n",
    "    ],\n",
    "    plot=False,\n",
    ")\n",
    "\n",
    "question = \"Give me an interesting code snippet from Notebook 5.\"\n",
    "question = \"Show me how the notebook explains diffusion. I believe it's part of the multimodal section.\"\n",
    "await stream_response(question, app, config, print_stream=False, show_meta=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431c8153",
   "metadata": {},
   "source": [
    "虽然这很有趣，但请注意，单靠这种策略会导致上下文超载。不过，这是朝着智能体方向迈出的坚实一步，并为构建有趣的状态管理应用提供了良好的实践参考。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d40a022b",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<br>\n",
    "\n",
    "### **7.5.4：继续使用 LangGraph？**\n",
    "\n",
    "在这门课之后，我们希望您能继续应用生成式 AI 和智能体范式，打造出惊人且具有影响力的系统！虽然我们没有在 LangGraph 上花太多时间，但非常鼓励您尝试阅读一些[**教程**](https://langchain-ai.github.io/langgraph/tutorials/)，并关注新出现的有趣智能体范式！\n",
    "\n",
    "<br>\n",
    "<hr>"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
