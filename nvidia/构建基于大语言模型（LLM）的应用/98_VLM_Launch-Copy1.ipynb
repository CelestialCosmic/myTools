{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4995ea03",
   "metadata": {},
   "source": [
    "<center><a href=\"https://www.nvidia.cn/training/\"><img src=\"https://dli-lms.s3.amazonaws.com/assets/general/DLI_Header_White.png\" width=\"400\" height=\"186\" /></a></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815ee2d5",
   "metadata": {},
   "source": [
    "# **98:** 启动您的 vLLM 服务\n",
    "\n",
    "请运行以下命令来启动一个运行本地视觉语言模型的 vLLM OpenAI 风格服务（在这个例子中是[**微软的 Phi-3.5-vision-instruct 模型**](https://huggingface.co/microsoft/Phi-3.5-vision-instruct)）。我们选择这个模型是因为它易于使用，相对快速，并且在 [**build.nvidia.com**](https://build.nvidia.com/microsoft/phi-3-vision-128k-instruct) 上可以获取到 NIM。\n",
    "\n",
    "这个过程会占用 Jupyter 事件循环，使得这个 notebook 被阻塞，但您可以在其它 notebook 中测试该入口。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f984ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !vllm serve microsoft/Phi-3.5-vision-128k-instruct \\\n",
    "\n",
    "!vllm serve microsoft/phi-3.5-vision-instruct  \\\n",
    "    --trust-remote-code \\\n",
    "    --max_model_len 16384 \\\n",
    "    --gpu-memory-utilization 0.8 \\\n",
    "    --enforce-eager \\\n",
    "    --port 9000\n",
    "    # --chat-template ./phi3.jinja \\\n",
    "\n",
    "## NOTE: \n",
    "# - trust-remote-code enabled because this model incorporates some custom code modules not found in basic huggingface transformers\n",
    "# - max-model-len set to ~16K since KV-cache takes up space and your GPU may start to run low if it's working with other processes\n",
    "# - gpu memory capped to avoid potential conflicts with other local processes\n",
    "# - more settings for more control, streamlines abstraction with other features offered by NIM-ified version"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
