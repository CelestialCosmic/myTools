{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91345992",
   "metadata": {},
   "source": [
    "<center><a href=\"https://www.nvidia.cn/training/\"><img src=\"https://dli-lms.s3.amazonaws.com/assets/general/DLI_Header_White.png\" width=\"400\" height=\"186\" /></a></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be40cd11",
   "metadata": {},
   "source": [
    "# 5. 评估"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41994abb",
   "metadata": {},
   "source": [
    "恭喜您完成今天的课程！希望这是一次有趣的旅程，收获了一些新技能。现在是时候把这些技能拿出来考验一下了。挑战来了：假设我们有一个分类模型，使用 LiDAR 数据来分类球体和立方体。相比于 RGB 摄像头，LiDAR 传感器并不那么容易获得，所以想将模型转换为能分类 RGB 图像的。由于使用了 [NVIDIA Omniverse](https://www.nvidia.com/en-us/omniverse/) 来生成 LiDAR 和 RGB 数据对，可以用这些数据创建一个对比预训练模型。由于 CLIP 已经被占用，我们将这个模型称为 `CILP`，代表“对比图像 LiDAR 预训练”。 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e23961",
   "metadata": {},
   "source": [
    "## 5.1 设置"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2944fb78",
   "metadata": {},
   "source": [
    "### 整体步骤：\n",
    "1. 加载已训练好的模型 lidar_cnn\n",
    "2. 用 lidar 和 rgb 训练 clip，再用这个 clip 嵌入 RGB，得到嵌入向量，经过 projector 转换后，得到新的 lidar_cnn\n",
    "3. 训练 projector，实现跨模态映射\n",
    "4. 构建新的图像分类模型 RGB2LiDARClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c8eacf",
   "metadata": {},
   "source": [
    "开始吧。以下是评估中使用的库。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86c1e86a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from assessment import assesment_utils\n",
    "from assessment.assesment_utils import Classifier\n",
    "import utils\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a40d8123",
   "metadata": {},
   "source": [
    "### 5.1.1 模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be667eb",
   "metadata": {},
   "source": [
    "接下来，让我们加载分类模型并命名为 `lidar_cnn`。如果我们抽出一点时间查看一下 [assement_utils](assessment/assesment_utils.py)，可以看到用于构建模型的 `Classifier` 类。请注意 `get_embs` 方法，我们将使用它来构建我们的跨模态映射器。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6943a29e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Classifier(\n",
       "  (embedder): Sequential(\n",
       "    (0): Conv2d(1, 50, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv2d(50, 100, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): ReLU()\n",
       "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (6): Conv2d(100, 200, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): ReLU()\n",
       "    (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (9): Conv2d(200, 200, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (10): ReLU()\n",
       "    (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (12): Flatten(start_dim=1, end_dim=-1)\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=3200, out_features=100, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=100, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lidar_cnn = Classifier(1).to(device)\n",
    "lidar_cnn.load_state_dict(torch.load(\"assessment/lidar_cnn.pt\", weights_only=True))\n",
    "# Do not unfreeze. Otherwise, it would be difficult to pass the assessment.\n",
    "for param in lidar_cnn.parameters():\n",
    "    lidar_cnn.requires_grad = False\n",
    "lidar_cnn.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0a26a3",
   "metadata": {},
   "source": [
    "### 5.1.2 数据集"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edcd4a87",
   "metadata": {},
   "source": [
    "以下是此评估中将使用的数据集。它与前几次实验中使用的数据集类似，但注意 `self.classes`。不同于第一节课的位置预测，这节我们将判断所评估的 RGB 或 LiDAR 是否包含 `cube` 或 `sphere`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "38364f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 64\n",
    "img_transforms = transforms.Compose([\n",
    "    transforms.Resize(IMG_SIZE),\n",
    "    transforms.ToTensor(),  # Scales data into [0,1]\n",
    "])\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, root_dir, start_idx, stop_idx):\n",
    "        self.classes = [\"cubes\", \"spheres\"]\n",
    "        self.root_dir = root_dir\n",
    "        self.rgb = []\n",
    "        self.lidar = []\n",
    "        self.class_idxs = []\n",
    "\n",
    "        for class_idx, class_name in enumerate(self.classes):\n",
    "            for idx in range(start_idx, stop_idx):\n",
    "                file_number = \"{:04d}\".format(idx)\n",
    "                rbg_img = Image.open(self.root_dir + class_name + \"/rgb/\" + file_number + \".png\")\n",
    "                rbg_img = img_transforms(rbg_img).to(device)\n",
    "                self.rgb.append(rbg_img)\n",
    "    \n",
    "                lidar_depth = np.load(self.root_dir + class_name + \"/lidar/\" + file_number + \".npy\")\n",
    "                lidar_depth = torch.from_numpy(lidar_depth[None, :, :]).to(torch.float32).to(device)\n",
    "                self.lidar.append(lidar_depth)\n",
    "\n",
    "                self.class_idxs.append(torch.tensor(class_idx, dtype=torch.float32)[None].to(device))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.class_idxs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        rbg_img = self.rgb[idx]\n",
    "        lidar_depth = self.lidar[idx]\n",
    "        class_idx = self.class_idxs[idx]\n",
    "        return rbg_img, lidar_depth, class_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "997e211a",
   "metadata": {},
   "source": [
    "这些数据可以在 `/data/assessment` 文件夹中找到。这里是一个立方体的示例。虽然图像比较小，但细节足以让我们的模型进行区分。\n",
    "\n",
    "<center><img src=\"data/assessment/cubes/rgb/0002.png\" /></center>\n",
    "\n",
    "接下来将数据加载到 `DataLoader` 中。我们会为验证留出一些批次（`VALID_BATCHES`）。其余的数据将用于训练。每种立方体和球体类别各有 `9999` 张图像，所以这里将 N 乘 2，以反映合并后的数据集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7d71512",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "VALID_BATCHES = 10\n",
    "N = 9999\n",
    "\n",
    "valid_N = VALID_BATCHES*BATCH_SIZE\n",
    "train_N = N - valid_N\n",
    "\n",
    "train_data = MyDataset(\"data/assessment/\", 0, train_N)\n",
    "train_dataloader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "valid_data = MyDataset(\"data/assessment/\", train_N, N)\n",
    "valid_dataloader = DataLoader(valid_data, batch_size=BATCH_SIZE, shuffle=False, drop_last=True)\n",
    "\n",
    "N *= 2\n",
    "valid_N *= 2\n",
    "train_N *= 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553adc4d",
   "metadata": {},
   "source": [
    "<a id=\"contrastive-pre-training\"></a>\n",
    "\n",
    "## 5.2 对比预训练"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3050c722",
   "metadata": {},
   "source": [
    "在创建跨模态映射模型之前，最好能有一个对 RGB 图像进行嵌入的方式。让我们利用数据，创建一个对比预训练模型。首先需要一个卷积模型，下面是推荐的架构。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d7d277c",
   "metadata": {},
   "outputs": [],
   "source": [
    "CILP_EMB_SIZE = 200\n",
    "\n",
    "class Embedder(nn.Module):\n",
    "    def __init__(self, in_ch, emb_size=CILP_EMB_SIZE):\n",
    "        super().__init__()\n",
    "        kernel_size = 3\n",
    "        stride = 1\n",
    "        padding = 1\n",
    "\n",
    "        # Convolution\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, 50, kernel_size, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(50, 100, kernel_size, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(100, 200, kernel_size, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(200, 200, kernel_size, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "\n",
    "        # Embeddings\n",
    "        self.dense_emb = nn.Sequential(\n",
    "            nn.Linear(200 * 4 * 4, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, emb_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        conv = self.conv(x)\n",
    "        emb = self.dense_emb(conv)\n",
    "        return F.normalize(emb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c54f05",
   "metadata": {},
   "source": [
    "RGB 数据有 `4` 个通道，而我们的 LiDAR 数据有 `1` 个通道。分别初始化这些嵌入模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e465b1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_embedder = Embedder(4).to(device)\n",
    "lidar_embedder = Embedder(1).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e24631",
   "metadata": {},
   "source": [
    "有嵌入模型了，现在把它们组合成一个 `ContrastivePretraining` 模型。\n",
    "\n",
    "**TODO**: 下面的 `ContrastivePretraining` 类几乎完成，但还有一些 `FIXME`。请替换这些 `FIXME`，以使模型能工作。您可以随时查看 notebook [02b_Contrastive_Pretraining.ipynb](02b_Contrastive_Pretraining.ipynb) 获取提示。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dddfd22b",
   "metadata": {},
   "source": [
    "混合模型需要加载两个模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4b0b33eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContrastivePretraining(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.img_embedder = img_embedder\n",
    "        self.lidar_embedder = lidar_embedder\n",
    "        self.cos = nn.CosineSimilarity()\n",
    "\n",
    "    def forward(self, rgb_imgs, lidar_depths):\n",
    "        img_emb = self.img_embedder(rgb_imgs)\n",
    "        lidar_emb = self.lidar_embedder(lidar_depths)\n",
    "\n",
    "        repeated_img_emb = img_emb.repeat_interleave(len(img_emb), dim=0)\n",
    "        repeated_lidar_emb = lidar_emb.repeat(len(lidar_emb), 1)\n",
    "\n",
    "        similarity = self.cos(repeated_img_emb, repeated_lidar_emb)\n",
    "        similarity = torch.unflatten(similarity, 0, (BATCH_SIZE, BATCH_SIZE))\n",
    "        similarity = (similarity + 1) / 2\n",
    "\n",
    "        logits_per_img = similarity\n",
    "        logits_per_lidar = similarity.T\n",
    "        return logits_per_img, logits_per_lidar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58bcb04b",
   "metadata": {},
   "source": [
    "是时候验证一下模型了！先初始化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "183414f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "CILP_model = ContrastivePretraining().to(device)\n",
    "optimizer = Adam(CILP_model.parameters(), lr=0.0001)\n",
    "loss_img = nn.CrossEntropyLoss()\n",
    "loss_lidar = nn.CrossEntropyLoss()\n",
    "ground_truth = torch.arange(BATCH_SIZE, dtype=torch.long).to(device)\n",
    "epochs = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd96fcb4",
   "metadata": {},
   "source": [
    "训练模型之前，应该定义一个损失函数来指导模型学习。\n",
    "\n",
    "**TODO**: 下面的 `get_CILP_loss` 函数也几乎完成。您还记得计算损失的公式吗？请替换下面的 `FIXME`。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef9d2411",
   "metadata": {},
   "source": [
    "由前面的代码加上变量倒推得到"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8b702c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_CILP_loss(batch):\n",
    "    rbg_img, lidar_depth, class_idx = batch\n",
    "    logits_per_img, logits_per_lidar = CILP_model(rbg_img, lidar_depth)\n",
    "    total_loss = (loss_img(logits_per_img, ground_truth) + loss_lidar(logits_per_lidar, ground_truth))/2\n",
    "    return total_loss, logits_per_img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f800a2",
   "metadata": {},
   "source": [
    "下面就该开始训练了。如果上面的 `TODO` 都正确完成，损失应该在 `3.2` 以下。看看对角线的值接近 `1` 吗？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "74a50a16",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "Train Loss: 3.0814396334722467 \n",
      "Similarity:\n",
      "tensor([[0.9961, 0.8826, 0.9635,  ..., 0.4457, 0.9774, 0.3969],\n",
      "        [0.8954, 0.9949, 0.9433,  ..., 0.2372, 0.9658, 0.2058],\n",
      "        [0.9508, 0.8960, 0.9951,  ..., 0.3770, 0.9709, 0.3361],\n",
      "        ...,\n",
      "        [0.4260, 0.1997, 0.3297,  ..., 0.9967, 0.3119, 0.9961],\n",
      "        [0.9024, 0.9299, 0.9854,  ..., 0.2893, 0.9593, 0.2568],\n",
      "        [0.3884, 0.1798, 0.3019,  ..., 0.9917, 0.2811, 0.9962]],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Valid Loss: 3.1908093251680074 \n",
      "Similarity:\n",
      "tensor([[0.9883, 0.7652, 0.4555,  ..., 0.5229, 0.5947, 0.9643],\n",
      "        [0.8368, 0.9938, 0.3124,  ..., 0.2135, 0.2656, 0.8444],\n",
      "        [0.3505, 0.2415, 0.9936,  ..., 0.7209, 0.6181, 0.2714],\n",
      "        ...,\n",
      "        [0.4660, 0.2214, 0.6955,  ..., 0.9977, 0.9824, 0.4480],\n",
      "        [0.5447, 0.2682, 0.5989,  ..., 0.9821, 0.9969, 0.5403],\n",
      "        [0.9913, 0.8119, 0.3122,  ..., 0.4702, 0.5682, 0.9970]],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Epoch 1\n",
      "Train Loss: 3.028641206905814 \n",
      "Similarity:\n",
      "tensor([[0.9389, 0.2091, 0.2838,  ..., 0.8347, 0.8414, 0.2893],\n",
      "        [0.2431, 0.9976, 0.4932,  ..., 0.2759, 0.4008, 0.5041],\n",
      "        [0.3116, 0.4220, 0.9872,  ..., 0.4205, 0.2813, 0.3731],\n",
      "        ...,\n",
      "        [0.9178, 0.2670, 0.3561,  ..., 0.9920, 0.5092, 0.3334],\n",
      "        [0.7603, 0.3657, 0.3331,  ..., 0.5062, 0.9951, 0.2906],\n",
      "        [0.2528, 0.4543, 0.3803,  ..., 0.2777, 0.2999, 0.9775]],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Valid Loss: 3.1807971377121773 \n",
      "Similarity:\n",
      "tensor([[0.9951, 0.7140, 0.3238,  ..., 0.4004, 0.5373, 0.9655],\n",
      "        [0.7055, 0.9976, 0.3712,  ..., 0.2766, 0.2939, 0.7474],\n",
      "        [0.3122, 0.3760, 0.9959,  ..., 0.5530, 0.4112, 0.2208],\n",
      "        ...,\n",
      "        [0.4483, 0.2633, 0.5984,  ..., 0.9978, 0.9679, 0.4406],\n",
      "        [0.5775, 0.2861, 0.4531,  ..., 0.9642, 0.9983, 0.5791],\n",
      "        [0.9822, 0.7586, 0.2153,  ..., 0.4237, 0.5716, 0.9985]],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Epoch 2\n",
      "Train Loss: 3.0224104674894416 \n",
      "Similarity:\n",
      "tensor([[0.9885, 0.3544, 0.4915,  ..., 0.3658, 0.7261, 0.4038],\n",
      "        [0.3589, 0.9942, 0.5686,  ..., 0.8322, 0.3692, 0.2310],\n",
      "        [0.5137, 0.5774, 0.9985,  ..., 0.3950, 0.1594, 0.4380],\n",
      "        ...,\n",
      "        [0.3807, 0.8144, 0.3979,  ..., 0.9978, 0.6461, 0.4773],\n",
      "        [0.6979, 0.3405, 0.1528,  ..., 0.6675, 0.9983, 0.7220],\n",
      "        [0.4088, 0.2185, 0.4399,  ..., 0.4945, 0.7219, 0.9977]],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Valid Loss: 3.176640598397506 \n",
      "Similarity:\n",
      "tensor([[0.9921, 0.6040, 0.3396,  ..., 0.4761, 0.6337, 0.9362],\n",
      "        [0.6560, 0.9988, 0.3868,  ..., 0.3094, 0.2820, 0.7357],\n",
      "        [0.2861, 0.3663, 0.9977,  ..., 0.6264, 0.4714, 0.1935],\n",
      "        ...,\n",
      "        [0.4817, 0.2937, 0.6164,  ..., 0.9982, 0.9597, 0.4751],\n",
      "        [0.6124, 0.2765, 0.4883,  ..., 0.9605, 0.9980, 0.5975],\n",
      "        [0.9774, 0.7195, 0.2101,  ..., 0.4782, 0.6276, 0.9979]],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    CILP_model.train()\n",
    "    train_loss = 0\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        loss, logits_per_img = get_CILP_loss(batch)\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "    assesment_utils.print_CILP_results(epoch, train_loss/step, logits_per_img, is_train=True)\n",
    "\n",
    "    CILP_model.eval()\n",
    "    valid_loss = 0\n",
    "    for step, batch in enumerate(valid_dataloader):\n",
    "        loss, logits_per_img = get_CILP_loss(batch)\n",
    "        valid_loss += loss.item()\n",
    "    assesment_utils.print_CILP_results(epoch, valid_loss/step, logits_per_img, is_train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee168c98",
   "metadata": {},
   "source": [
    "完成后，请冻结模型。后面将使用这个模型和跨模型映射模型进行评估。如果跨模型映射训练期间修改了这个模型的话，可能就没法通过评估了！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5b046016",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in CILP_model.parameters():\n",
    "    CILP_model.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f1c03e",
   "metadata": {},
   "source": [
    "## 5.3 跨模态映射"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c259678f",
   "metadata": {},
   "source": [
    "现在有嵌入图像数据的方法了，接下来就来做跨模态映射吧。\n",
    "\n",
    "**TODO**: 让我们直接开始，创建映射器。模型的输入维度应该是什么，输出维度又应该是什么呢？关于第一个 `FIXME` 的提示可以在 `Embedder` 类的[#5.2 对比预训练](#contrastive-pre-training)找到。第二个 `FIXME` 的提示可以在 `assessment/assesment_utils.py` 文件的 `Classifier` 类中找到。第二个 `FIXME` 的维度应该和 `get_embs` 函数的输出大小一致。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec4b1fb",
   "metadata": {},
   "source": [
    "线性代数知识：矩阵乘法要求第一个矩阵的列数必须等于第二个矩阵的行数\n",
    "报错提示前者为 200\n",
    "由代码得到后者答案为 3200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd9dc5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "self.classifier = nn.Sequential(\n",
    "            nn.Linear(200 * 4 * 4, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, n_classes)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a9402d",
   "metadata": {},
   "outputs": [],
   "source": [
    "projector = nn.Sequential(\n",
    "    nn.Linear(200, 1000),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(1000, 500),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(500, 3200)\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638fcf73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 输出\n",
    "Classifier(\n",
    "  (embedder): Sequential(\n",
    "    (0): Conv2d(1, 50, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "    (1): ReLU()\n",
    "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
    "    (3): Conv2d(50, 100, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "    (4): ReLU()\n",
    "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
    "    (6): Conv2d(100, 200, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "    (7): ReLU()\n",
    "    (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
    "    (9): Conv2d(200, 200, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "    (10): ReLU()\n",
    "    (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
    "    (12): Flatten(start_dim=1, end_dim=-1)\n",
    "  )\n",
    "  (classifier): Sequential(\n",
    "    (0): Linear(in_features=3200, out_features=100, bias=True)\n",
    "    (1): ReLU()\n",
    "    (2): Linear(in_features=100, out_features=1, bias=True)\n",
    "  )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f794b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "projector = nn.Sequential(\n",
    "    nn.Linear(clip_emb_size[0]*patches, 100),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(100, 100),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(100, vgg_shape[0])\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c6e3d8",
   "metadata": {},
   "source": [
    "输入是 clip 需要的，输出是 vgg 需要的，中间自己定\n",
    "转为新向量后作为分类头送去训练\n",
    "clip -> projector -> classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "279fb25e",
   "metadata": {},
   "source": [
    "接下来，定义训练 `projector` 的损失函数。\n",
    "\n",
    "**TODO**: 用于估计映射嵌入的损失函数是什么？请替换下面的 `FIXME`。可以查看 notebook [03a_Projection.ipynb](03a_Projection.ipynb) 的 3.2 部分获得提示。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e93b93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_projector_loss(model, batch):\n",
    "    rbg_img, lidar_depth, class_idx = batch\n",
    "    imb_embs = CILP_model.img_embedder(rbg_img)\n",
    "    lidar_emb = lidar_cnn.get_embs(lidar_depth)\n",
    "    pred_lidar_embs = model(imb_embs)\n",
    "    return nn.FIXME()(pred_lidar_embs, lidar_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb492da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_projector_loss(model, batch):\n",
    "    imgs, texts, _ = batch\n",
    "    imb_embs = flower_classifier.get_img_embs(imgs)\n",
    "\n",
    "    text_encodings = get_clip_encodings(texts)\n",
    "    pred_img_embs = model(text_encodings).to(device)\n",
    "    return nn.MSELoss()(pred_img_embs, imb_embs), 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "069ce585",
   "metadata": {},
   "source": [
    "训练 `projector` 会花费一些时间，最终验证损失应该达到 2 左右。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da10b98",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "epochs = 40\n",
    "optimizer = torch.optim.Adam(projector.parameters())\n",
    "assesment_utils.train_model(projector, optimizer, get_projector_loss, epochs, train_dataloader, valid_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be7ec31",
   "metadata": {},
   "source": [
    "是时候把它们合一起了。创建一个新模型 `RGB2LiDARClassifier`，用上映射器和预训练的 `lidar_cnn` 模型。\n",
    "\n",
    "**TODO**: 请填上 `FIXME`。现在应该用 `CILP_model` 的哪个 `embedder`？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef441602",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RGB2LiDARClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.projector = projector\n",
    "        self.FIXME = CILP_model.FIXME_embedder\n",
    "        self.shape_classifier = lidar_cnn\n",
    "    \n",
    "    def forward(self, imgs):\n",
    "        img_encodings = self.img_embedder(imgs)\n",
    "        proj_lidar_embs = self.projector(img_encodings)\n",
    "        return self.shape_classifier(data_embs=proj_lidar_embs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28dbb82d",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_classifier = RGB2LiDARClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63eebefb",
   "metadata": {},
   "source": [
    "在训练这个模型之前，先看看它开箱即用的效果。我们创建一个 `get_correct` 函数，用于计算正确分类的数量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0b4afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_correct(output, y):\n",
    "    zero_tensor = torch.tensor([0]).to(device)\n",
    "    pred = torch.gt(output, zero_tensor)\n",
    "    correct = pred.eq(y.view_as(pred)).sum().item()\n",
    "    return correct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c28c1f",
   "metadata": {},
   "source": [
    "接下来用一个 `get_valid_metrics` 函数，计算模型在验证数集的准确率。如果做对了，准确率应该超过 `.70`，也就是 70%。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a2e104",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_valid_metrics():\n",
    "    my_classifier.eval()\n",
    "    correct = 0\n",
    "    batch_correct = 0\n",
    "    for step, batch in enumerate(valid_dataloader):\n",
    "        rbg_img, _, class_idx = batch\n",
    "        output = my_classifier(rbg_img)\n",
    "        loss = nn.BCEWithLogitsLoss()(output, class_idx)\n",
    "        batch_correct = get_correct(output, class_idx)\n",
    "        correct += batch_correct\n",
    "    print(f\"Valid Loss: {loss.item():2.4f} | Accuracy {correct/valid_N:2.4f}\")\n",
    "\n",
    "get_valid_metrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53aa7fbf",
   "metadata": {},
   "source": [
    "最后，让我们微调完成的模型。因为 `CILP` 和 `lidar_cnn` 都被冻结，所以这只会改变模型的 `projector` 部分。模型的验证准确率也应该能超过 `.95` 也就是 95%。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0810058b",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "optimizer = torch.optim.Adam(my_classifier.parameters())\n",
    "\n",
    "my_classifier.train()\n",
    "for epoch in range(epochs):\n",
    "    correct = 0\n",
    "    batch_correct = 0\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        rbg_img, _, class_idx = batch\n",
    "        output = my_classifier(rbg_img)\n",
    "        loss = nn.BCEWithLogitsLoss()(output, class_idx)\n",
    "        batch_correct = get_correct(output, class_idx)\n",
    "        correct += batch_correct\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"Train Loss: {loss.item():2.4f} | Accuracy {correct/train_N:2.4f}\")\n",
    "    get_valid_metrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01da0311",
   "metadata": {},
   "source": [
    "## 5.4 运行评估"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d0969e",
   "metadata": {},
   "source": [
    "为了评估您的模型，请运行以下两个单元格。评估满分为 10 分：\n",
    "\n",
    "* 确保 CILP 的验证损失低于 `3.2`（5 分）\n",
    "* 确保 `projector` 可以与 `lidar_cnn` 一起使用，准确分类图像。如果准确率超过 `.95`，将测试五个批次的图像。（每个批次 1 分，总共 5 分）\n",
    "\n",
    "您需要获得 10 分中的 9 分才能通过评估。祝您好运！\n",
    "\n",
    "请在下面提交您的 `CILP_model` 和 `projector`。如果这些模型的名称已经更改，请相应更新。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "390aa333",
   "metadata": {},
   "outputs": [],
   "source": [
    "from run_assessment import run_assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215970fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_assessment(CILP_model, projector)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "269eb378",
   "metadata": {},
   "source": [
    "## 6.7 生成证书"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d912771f",
   "metadata": {},
   "source": [
    "如果您通过了评估，请返回课程页面并点击 \"ASSESS TASK\" 按钮，这将为您生成课程证书。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
